{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e51a0d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Instalar dependencias faltantes (ejecuta solo si es necesario)\n",
    "%pip install -q transformers accelerate tiktoken protobuf sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c9b46d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello1\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello1\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ac7e8a",
   "metadata": {},
   "source": [
    "## 📚 Imports y Configuración - DeBERTa-v3-base\n",
    "\n",
    "Implementación completa con todas las técnicas avanzadas recomendadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6928786e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Configuración cargada\n",
      "   Device: cpu\n",
      "   Modelo: microsoft/deberta-v3-base\n",
      "   Max length: 128\n",
      "   Batch size: 16\n",
      "\n",
      "✅ Todas las funciones cargadas correctamente\n",
      "🚀 Listo para entrenar DeBERTa-v3-base con técnicas avanzadas\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# IMPORTS Y CONFIGURACIÓN INICIAL - DEBERTA-V3-BASE\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    DebertaV2Tokenizer,\n",
    "    TrainingArguments, Trainer, EarlyStoppingCallback,\n",
    "    set_seed\n",
    ")\n",
    "\n",
    "# Desactivar warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "set_seed(42)\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURACIÓN GLOBAL\n",
    "# ============================================================================\n",
    "\n",
    "class Config:\n",
    "    # Modelo\n",
    "    MODEL_NAME = \"microsoft/deberta-v3-base\"\n",
    "    MAX_LEN = 128  # Versos cortos (96-128)\n",
    "    NUM_LABELS = 3  # Excluyendo clase 3 (mixed)\n",
    "\n",
    "    # Entrenamiento\n",
    "    BATCH_SIZE = 16\n",
    "    LEARNING_RATE = 2e-5  # 1e-5 a 2e-5\n",
    "    NUM_EPOCHS = 6  # 3-6 épocas\n",
    "    WARMUP_RATIO = 0.05  # 5% warmup\n",
    "    WEIGHT_DECAY = 0.01\n",
    "\n",
    "    # Regularización\n",
    "    LABEL_SMOOTHING = 0.1  # 0.05-0.1\n",
    "    FOCAL_GAMMA = 1.5  # γ=1-2 para Focal Loss\n",
    "    R_DROP_ALPHA = 4.0  # α≈4 para R-Drop\n",
    "\n",
    "    # Validación\n",
    "    N_FOLDS = 5  # Stratified k-fold\n",
    "    EARLY_STOPPING_PATIENCE = 3\n",
    "\n",
    "    # Device\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "config = Config()\n",
    "\n",
    "print(f\"✅ Configuración cargada\")\n",
    "print(f\"   Device: {config.DEVICE}\")\n",
    "print(f\"   Modelo: {config.MODEL_NAME}\")\n",
    "print(f\"   Max length: {config.MAX_LEN}\")\n",
    "print(f\"   Batch size: {config.BATCH_SIZE}\")\n",
    "\n",
    "# ============================================================================\n",
    "# PREPROCESAMIENTO (sin stemming/stopwords - preservar matices)\n",
    "# ============================================================================\n",
    "\n",
    "def advanced_text_preprocessing(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Preprocesamiento para texto poético:\n",
    "    - Normalizar comillas y guiones\n",
    "    - NO stemming/stopwords (preservar matices)\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    # Normalizar comillas dobles\n",
    "    text = text.replace('\"', '\"').replace('\"', '\"')\n",
    "    \n",
    "    # Normalizar comillas simples\n",
    "    text = text.replace(''', \"'\").replace(''', \"'\")\n",
    "\n",
    "    # Normalizar guiones\n",
    "    text = text.replace('–', '-').replace('—', '-')\n",
    "\n",
    "    # Eliminar espacios múltiples\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "def remove_duplicate_verses(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Eliminar versos duplicados.\"\"\"\n",
    "    initial = len(df)\n",
    "    df_clean = df.drop_duplicates(subset=['verse_text'], keep='first')\n",
    "    print(f\"   Eliminados {initial - len(df_clean)} versos duplicados\")\n",
    "    return df_clean\n",
    "\n",
    "# ============================================================================\n",
    "# FOCAL LOSS PARA DESBALANCE\n",
    "# ============================================================================\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"Focal Loss con class weights.\"\"\"\n",
    "    def __init__(self, alpha=None, gamma=config.FOCAL_GAMMA, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none', weight=self.alpha)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = ((1 - pt) ** self.gamma) * ce_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        return focal_loss\n",
    "\n",
    "# ============================================================================\n",
    "# R-DROP REGULARIZATION\n",
    "# ============================================================================\n",
    "\n",
    "def r_drop_loss(logits1, logits2, targets, alpha=config.R_DROP_ALPHA):\n",
    "    \"\"\"R-Drop: α≈4 para mejor generalización.\"\"\"\n",
    "    loss1 = F.cross_entropy(logits1, targets)\n",
    "    loss2 = F.cross_entropy(logits2, targets)\n",
    "    \n",
    "    kl_loss = F.kl_div(\n",
    "        F.log_softmax(logits1, dim=-1),\n",
    "        F.softmax(logits2, dim=-1),\n",
    "        reduction='batchmean'\n",
    "    )\n",
    "    \n",
    "    return (loss1 + loss2) / 2 + alpha * kl_loss\n",
    "\n",
    "# ============================================================================\n",
    "# DATASET CUSTOM\n",
    "# ============================================================================\n",
    "\n",
    "class PoemDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=config.MAX_LEN):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# ============================================================================\n",
    "# CUSTOM TRAINER CON FOCAL LOSS Y R-DROP\n",
    "# ============================================================================\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, class_weights=None, use_r_drop=False, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = class_weights\n",
    "        self.use_r_drop = use_r_drop\n",
    "\n",
    "        if class_weights is not None:\n",
    "            self.criterion = FocalLoss(alpha=class_weights.to(self.model.device))\n",
    "        else:\n",
    "            self.criterion = FocalLoss()\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "\n",
    "        if self.use_r_drop:\n",
    "            outputs1 = model(**inputs)\n",
    "            outputs2 = model(**inputs)\n",
    "            loss = r_drop_loss(outputs1.logits, outputs2.logits, labels)\n",
    "        else:\n",
    "            outputs = model(**inputs)\n",
    "            loss = self.criterion(outputs.logits, labels)\n",
    "\n",
    "        return (loss, outputs if not self.use_r_drop else outputs1) if return_outputs else loss\n",
    "\n",
    "# ============================================================================\n",
    "# MÉTRICAS\n",
    "# ============================================================================\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy_score(labels, predictions),\n",
    "        'f1_macro': f1_score(labels, predictions, average='macro'),\n",
    "        'f1_weighted': f1_score(labels, predictions, average='weighted')\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# STRATIFIED K-FOLD TRAINING\n",
    "# ============================================================================\n",
    "\n",
    "def stratified_kfold_training(train_texts, train_labels, config):\n",
    "    \"\"\"Entrenamiento con Stratified K-Fold (5 folds).\"\"\"\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=config.N_FOLDS, shuffle=True, random_state=42)\n",
    "    fold_results = []\n",
    "    best_models = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(train_texts, train_labels)):\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"FOLD {fold + 1}/{config.N_FOLDS}\")\n",
    "        print(f\"{'='*50}\")\n",
    "\n",
    "        # Split data\n",
    "        fold_train_texts = [train_texts[i] for i in train_idx]\n",
    "        fold_train_labels = [train_labels[i] for i in train_idx]\n",
    "        fold_val_texts = [train_texts[i] for i in val_idx]\n",
    "        fold_val_labels = [train_labels[i] for i in val_idx]\n",
    "\n",
    "        # Class weights\n",
    "        class_weights = compute_class_weight(\n",
    "            'balanced',\n",
    "            classes=np.unique(fold_train_labels),\n",
    "            y=fold_train_labels\n",
    "        )\n",
    "        class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
    "        \n",
    "        # Tokenizer y modelo\n",
    "        tokenizer = AutoTokenizer.from_pretrained(config.MODEL_NAME)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            config.MODEL_NAME,\n",
    "            num_labels=config.NUM_LABELS\n",
    "        )\n",
    "\n",
    "        # Datasets\n",
    "        train_dataset = PoemDataset(fold_train_texts, fold_train_labels, tokenizer)\n",
    "        val_dataset = PoemDataset(fold_val_texts, fold_val_labels, tokenizer)\n",
    "\n",
    "        # Training arguments (AdamW, cosine decay, warmup 5%)\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=f\"./results/fold_{fold}\",\n",
    "            num_train_epochs=config.NUM_EPOCHS,\n",
    "            per_device_train_batch_size=config.BATCH_SIZE,\n",
    "            per_device_eval_batch_size=config.BATCH_SIZE * 2,\n",
    "            learning_rate=config.LEARNING_RATE,\n",
    "            weight_decay=config.WEIGHT_DECAY,\n",
    "            warmup_ratio=config.WARMUP_RATIO,\n",
    "            lr_scheduler_type=\"cosine\",\n",
    "            eval_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"f1_macro\",\n",
    "            greater_is_better=True,\n",
    "            logging_steps=50,\n",
    "            report_to=\"none\",\n",
    "            label_smoothing_factor=config.LABEL_SMOOTHING\n",
    "        )\n",
    "\n",
    "        # Trainer con R-Drop\n",
    "        trainer = CustomTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            tokenizer=tokenizer,\n",
    "            compute_metrics=compute_metrics,\n",
    "            callbacks=[EarlyStoppingCallback(early_stopping_patience=config.EARLY_STOPPING_PATIENCE)],\n",
    "            class_weights=class_weights,\n",
    "            use_r_drop=True\n",
    "        )\n",
    "\n",
    "        # Entrenar\n",
    "        trainer.train()\n",
    "\n",
    "        # Evaluar\n",
    "        eval_results = trainer.evaluate()\n",
    "        fold_results.append(eval_results)\n",
    "        best_models.append(trainer.model)\n",
    "\n",
    "        print(f\"Fold {fold + 1} - F1 Macro: {eval_results['eval_f1_macro']:.4f}\")\n",
    "\n",
    "    return fold_results, best_models\n",
    "\n",
    "# ============================================================================\n",
    "# ENSEMBLE DE LOGITS\n",
    "# ============================================================================\n",
    "\n",
    "def ensemble_predictions(models, tokenizer, test_texts, config):\n",
    "    \"\"\"Ensamblar logits de múltiples checkpoints.\"\"\"\n",
    "    all_predictions = []\n",
    "\n",
    "    test_dataset = PoemDataset(test_texts, [0]*len(test_texts), tokenizer)\n",
    "\n",
    "    for model in models:\n",
    "        model.eval()\n",
    "        model.to(config.DEVICE)\n",
    "        predictions = []\n",
    "\n",
    "        dataloader = DataLoader(test_dataset, batch_size=config.BATCH_SIZE * 4, shuffle=False)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                input_ids = batch['input_ids'].to(config.DEVICE)\n",
    "                attention_mask = batch['attention_mask'].to(config.DEVICE)\n",
    "\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                predictions.extend(outputs.logits.cpu().numpy())\n",
    "\n",
    "        all_predictions.append(np.array(predictions))\n",
    "\n",
    "    # Promediar logits\n",
    "    ensemble_logits = np.mean(all_predictions, axis=0)\n",
    "    ensemble_preds = np.argmax(ensemble_logits, axis=1)\n",
    "\n",
    "    return ensemble_preds, ensemble_logits\n",
    "\n",
    "print(\"\\n✅ Todas las funciones cargadas correctamente\")\n",
    "print(\"🚀 Listo para entrenar DeBERTa-v3-base con técnicas avanzadas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42914682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Evaluando en conjunto de test con ensemble de modelos...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'best_models' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      8\u001b[39m tokenizer = AutoTokenizer.from_pretrained(config.MODEL_NAME)\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Ensemble predictions\u001b[39;00m\n\u001b[32m     11\u001b[39m ensemble_preds, ensemble_logits = ensemble_predictions(\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     \u001b[43mbest_models\u001b[49m, \n\u001b[32m     13\u001b[39m     tokenizer, \n\u001b[32m     14\u001b[39m     test_texts, \n\u001b[32m     15\u001b[39m     config\n\u001b[32m     16\u001b[39m )\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Métricas finales\u001b[39;00m\n\u001b[32m     19\u001b[39m final_accuracy = accuracy_score(test_labels, ensemble_preds)\n",
      "\u001b[31mNameError\u001b[39m: name 'best_models' is not defined"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# EVALUACIÓN FINAL EN TEST SET CON ENSEMBLE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"🎯 Evaluando en conjunto de test con ensemble de modelos...\")\n",
    "\n",
    "# Cargar tokenizer (usaremos el del último fold, pero todos son iguales)\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.MODEL_NAME)\n",
    "\n",
    "# Ensemble predictions\n",
    "ensemble_preds, ensemble_logits = ensemble_predictions(\n",
    "    best_models, \n",
    "    tokenizer, \n",
    "    test_texts, \n",
    "    config\n",
    ")\n",
    "\n",
    "# Métricas finales\n",
    "final_accuracy = accuracy_score(test_labels, ensemble_preds)\n",
    "final_f1_macro = f1_score(test_labels, ensemble_preds, average='macro')\n",
    "final_f1_weighted = f1_score(test_labels, ensemble_preds, average='weighted')\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"🏆 RESULTADOS FINALES EN TEST SET\")\n",
    "print(\"=\"*70)\n",
    "print(f\"✅ F1 Macro (Objetivo: ≥0.85):    {final_f1_macro:.4f}\")\n",
    "print(f\"   F1 Weighted:                   {final_f1_weighted:.4f}\")\n",
    "print(f\"   Accuracy:                      {final_accuracy:.4f}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if final_f1_macro >= 0.85:\n",
    "    print(\"🎉 ¡OBJETIVO ALCANZADO! F1-macro ≥ 0.85\")\n",
    "else:\n",
    "    gap = 0.85 - final_f1_macro\n",
    "    print(f\"⚠️  Falta {gap:.4f} para alcanzar el objetivo de 0.85\")\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\n📋 Classification Report:\")\n",
    "print(\"-\" * 70)\n",
    "print(classification_report(test_labels, ensemble_preds, digits=4))\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"\\n🔍 Confusion Matrix:\")\n",
    "cm = confusion_matrix(test_labels, ensemble_preds)\n",
    "print(cm)\n",
    "\n",
    "# Visualización\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Confusion Matrix Heatmap\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0])\n",
    "axes[0].set_title('Confusion Matrix - Test Set')\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('True')\n",
    "\n",
    "# F1 Scores por clase\n",
    "class_f1_scores = f1_score(test_labels, ensemble_preds, average=None)\n",
    "axes[1].bar(range(len(class_f1_scores)), class_f1_scores, color=['#2ecc71', '#e74c3c', '#3498db'])\n",
    "axes[1].set_xlabel('Clase')\n",
    "axes[1].set_ylabel('F1 Score')\n",
    "axes[1].set_title('F1 Score por Clase - Test Set')\n",
    "axes[1].set_xticks(range(len(class_f1_scores)))\n",
    "axes[1].axhline(y=0.85, color='red', linestyle='--', label='Objetivo (0.85)')\n",
    "axes[1].legend()\n",
    "axes[1].set_ylim([0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✅ Evaluación completada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b67d7a",
   "metadata": {},
   "source": [
    "## 🎯 Evaluación Final con Ensemble\n",
    "\n",
    "Evaluar en el conjunto de test usando ensemble de los 5 mejores modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae9998ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Iniciando entrenamiento con Stratified K-Fold...\n",
      "⚙️  Configuración:\n",
      "   Modelo: microsoft/deberta-v3-base\n",
      "   Folds: 5\n",
      "   Epochs: 6\n",
      "   Batch size: 16\n",
      "   Learning rate: 2e-05\n",
      "   Focal Loss γ: 1.5\n",
      "   R-Drop α: 4.0\n",
      "   Label smoothing: 0.1\n",
      "   Early stopping patience: 3\n",
      "\n",
      "======================================================================\n",
      "\n",
      "==================================================\n",
      "FOLD 1/5\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 2, 'bos_token_id': 1}.\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 2, 'bos_token_id': 1}.\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Exception ignored while calling deallocator <function tqdm.__del__ at 0x0000019E1C934B40>:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\osorn\\anaconda3\\envs\\dipEnv\\Lib\\site-packages\\tqdm\\std.py\", line 1148, in __del__\n",
      "    self.close()\n",
      "  File \"c:\\Users\\osorn\\anaconda3\\envs\\dipEnv\\Lib\\site-packages\\tqdm\\notebook.py\", line 279, in close\n",
      "    self.disp(bar_style='danger', check_delay=False)\n",
      "AttributeError: 'tqdm' object has no attribute 'disp'\n",
      "Exception ignored while calling deallocator <function tqdm.__del__ at 0x0000019E1C934B40>:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\osorn\\anaconda3\\envs\\dipEnv\\Lib\\site-packages\\tqdm\\std.py\", line 1148, in __del__\n",
      "    self.close()\n",
      "  File \"c:\\Users\\osorn\\anaconda3\\envs\\dipEnv\\Lib\\site-packages\\tqdm\\notebook.py\", line 279, in close\n",
      "    self.disp(bar_style='danger', check_delay=False)\n",
      "AttributeError: 'tqdm' object has no attribute 'disp'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='288' max='288' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [288/288 59:14, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.853617</td>\n",
       "      <td>0.657895</td>\n",
       "      <td>0.264550</td>\n",
       "      <td>0.522139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.947100</td>\n",
       "      <td>0.622136</td>\n",
       "      <td>0.768421</td>\n",
       "      <td>0.542468</td>\n",
       "      <td>0.700595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.742800</td>\n",
       "      <td>0.471198</td>\n",
       "      <td>0.857895</td>\n",
       "      <td>0.818079</td>\n",
       "      <td>0.855608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.474700</td>\n",
       "      <td>0.444519</td>\n",
       "      <td>0.826316</td>\n",
       "      <td>0.789629</td>\n",
       "      <td>0.828775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.322300</td>\n",
       "      <td>0.393939</td>\n",
       "      <td>0.873684</td>\n",
       "      <td>0.839238</td>\n",
       "      <td>0.872265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.218200</td>\n",
       "      <td>0.398452</td>\n",
       "      <td>0.868421</td>\n",
       "      <td>0.835113</td>\n",
       "      <td>0.867540</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6/6 00:38]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 - F1 Macro: 0.8392\n",
      "\n",
      "==================================================\n",
      "FOLD 2/5\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 2, 'bos_token_id': 1}.\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 2, 'bos_token_id': 1}.\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Exception ignored while calling deallocator <function tqdm.__del__ at 0x0000019E1C934B40>:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\osorn\\anaconda3\\envs\\dipEnv\\Lib\\site-packages\\tqdm\\std.py\", line 1148, in __del__\n",
      "    self.close()\n",
      "  File \"c:\\Users\\osorn\\anaconda3\\envs\\dipEnv\\Lib\\site-packages\\tqdm\\notebook.py\", line 279, in close\n",
      "    self.disp(bar_style='danger', check_delay=False)\n",
      "AttributeError: 'tqdm' object has no attribute 'disp'\n",
      "Exception ignored while calling deallocator <function tqdm.__del__ at 0x0000019E1C934B40>:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\osorn\\anaconda3\\envs\\dipEnv\\Lib\\site-packages\\tqdm\\std.py\", line 1148, in __del__\n",
      "    self.close()\n",
      "  File \"c:\\Users\\osorn\\anaconda3\\envs\\dipEnv\\Lib\\site-packages\\tqdm\\notebook.py\", line 279, in close\n",
      "    self.disp(bar_style='danger', check_delay=False)\n",
      "AttributeError: 'tqdm' object has no attribute 'disp'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='288' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 13/288 02:22 < 59:18, 0.08 it/s, Epoch 0.25/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m70\u001b[39m)\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# NOTA: Este entrenamiento puede tomar VARIAS HORAS en CPU\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Cada fold con 6 épocas puede tomar ~1-2 horas\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Total estimado: 5-10 horas para 5 folds\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m fold_results, best_models = \u001b[43mstratified_kfold_training\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_texts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m70\u001b[39m)\n\u001b[32m     29\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✅ Entrenamiento K-Fold completado!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 295\u001b[39m, in \u001b[36mstratified_kfold_training\u001b[39m\u001b[34m(train_texts, train_labels, config)\u001b[39m\n\u001b[32m    282\u001b[39m trainer = CustomTrainer(\n\u001b[32m    283\u001b[39m     model=model,\n\u001b[32m    284\u001b[39m     args=training_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m    291\u001b[39m     use_r_drop=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    292\u001b[39m )\n\u001b[32m    294\u001b[39m \u001b[38;5;66;03m# Entrenar\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m295\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[38;5;66;03m# Evaluar\u001b[39;00m\n\u001b[32m    298\u001b[39m eval_results = trainer.evaluate()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\osorn\\anaconda3\\envs\\dipEnv\\Lib\\site-packages\\transformers\\trainer.py:2325\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2323\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2324\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2326\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2327\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\osorn\\anaconda3\\envs\\dipEnv\\Lib\\site-packages\\transformers\\trainer.py:2674\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2667\u001b[39m context = (\n\u001b[32m   2668\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2669\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2670\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2671\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2672\u001b[39m )\n\u001b[32m   2673\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2674\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2676\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2677\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2678\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2679\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2680\u001b[39m ):\n\u001b[32m   2681\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2682\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\osorn\\anaconda3\\envs\\dipEnv\\Lib\\site-packages\\transformers\\trainer.py:4020\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(self, model, inputs, num_items_in_batch)\u001b[39m\n\u001b[32m   4017\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb.reduce_mean().detach().to(\u001b[38;5;28mself\u001b[39m.args.device)\n\u001b[32m   4019\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.compute_loss_context_manager():\n\u001b[32m-> \u001b[39m\u001b[32m4020\u001b[39m     loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4022\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[32m   4023\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   4024\u001b[39m     \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4025\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.global_step % \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps == \u001b[32m0\u001b[39m\n\u001b[32m   4026\u001b[39m ):\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 198\u001b[39m, in \u001b[36mCustomTrainer.compute_loss\u001b[39m\u001b[34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[39m\n\u001b[32m    196\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.use_r_drop:\n\u001b[32m    197\u001b[39m     outputs1 = model(**inputs)\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m     outputs2 = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    199\u001b[39m     loss = r_drop_loss(outputs1.logits, outputs2.logits, labels)\n\u001b[32m    200\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\osorn\\anaconda3\\envs\\dipEnv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\osorn\\anaconda3\\envs\\dipEnv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\osorn\\anaconda3\\envs\\dipEnv\\Lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:1077\u001b[39m, in \u001b[36mDebertaV2ForSequenceClassification.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1069\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1070\u001b[39m \u001b[33;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[32m   1071\u001b[39m \u001b[33;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[32m   1072\u001b[39m \u001b[33;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[32m   1073\u001b[39m \u001b[33;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[32m   1074\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1075\u001b[39m return_dict = return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_return_dict\n\u001b[32m-> \u001b[39m\u001b[32m1077\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdeberta\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1078\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1079\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1080\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1081\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1082\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1083\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1084\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1085\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1086\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1088\u001b[39m encoder_layer = outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1089\u001b[39m pooled_output = \u001b[38;5;28mself\u001b[39m.pooler(encoder_layer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\osorn\\anaconda3\\envs\\dipEnv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\osorn\\anaconda3\\envs\\dipEnv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\osorn\\anaconda3\\envs\\dipEnv\\Lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:784\u001b[39m, in \u001b[36mDebertaV2Model.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    774\u001b[39m     token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n\u001b[32m    776\u001b[39m embedding_output = \u001b[38;5;28mself\u001b[39m.embeddings(\n\u001b[32m    777\u001b[39m     input_ids=input_ids,\n\u001b[32m    778\u001b[39m     token_type_ids=token_type_ids,\n\u001b[32m   (...)\u001b[39m\u001b[32m    781\u001b[39m     inputs_embeds=inputs_embeds,\n\u001b[32m    782\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m784\u001b[39m encoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    785\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    786\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    787\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    791\u001b[39m encoded_layers = encoder_outputs[\u001b[32m1\u001b[39m]\n\u001b[32m    793\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.z_steps > \u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\osorn\\anaconda3\\envs\\dipEnv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\osorn\\anaconda3\\envs\\dipEnv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\osorn\\anaconda3\\envs\\dipEnv\\Lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:657\u001b[39m, in \u001b[36mDebertaV2Encoder.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, output_hidden_states, output_attentions, query_states, relative_pos, return_dict)\u001b[39m\n\u001b[32m    655\u001b[39m rel_embeddings = \u001b[38;5;28mself\u001b[39m.get_rel_embedding()\n\u001b[32m    656\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, layer_module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m.layer):\n\u001b[32m--> \u001b[39m\u001b[32m657\u001b[39m     output_states, attn_weights = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnext_kv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    659\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    660\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    661\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    662\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    663\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    664\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    666\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[32m    667\u001b[39m         all_attentions = all_attentions + (attn_weights,)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\osorn\\anaconda3\\envs\\dipEnv\\Lib\\site-packages\\transformers\\modeling_layers.py:94\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     91\u001b[39m         logger.warning_once(message)\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\osorn\\anaconda3\\envs\\dipEnv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\osorn\\anaconda3\\envs\\dipEnv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\osorn\\anaconda3\\envs\\dipEnv\\Lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:436\u001b[39m, in \u001b[36mDebertaV2Layer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, query_states, relative_pos, rel_embeddings, output_attentions)\u001b[39m\n\u001b[32m    427\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    428\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    429\u001b[39m     hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    434\u001b[39m     output_attentions: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    435\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[torch.Tensor, Optional[torch.Tensor]]:\n\u001b[32m--> \u001b[39m\u001b[32m436\u001b[39m     attention_output, att_matrix = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    437\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    438\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    440\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    441\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    442\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    443\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    444\u001b[39m     intermediate_output = \u001b[38;5;28mself\u001b[39m.intermediate(attention_output)\n\u001b[32m    445\u001b[39m     layer_output = \u001b[38;5;28mself\u001b[39m.output(intermediate_output, attention_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\osorn\\anaconda3\\envs\\dipEnv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\osorn\\anaconda3\\envs\\dipEnv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\osorn\\anaconda3\\envs\\dipEnv\\Lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:379\u001b[39m, in \u001b[36mDebertaV2Attention.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[39m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m query_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    378\u001b[39m     query_states = hidden_states\n\u001b[32m--> \u001b[39m\u001b[32m379\u001b[39m attention_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mself_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    381\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[32m    382\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (attention_output, att_matrix)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\osorn\\anaconda3\\envs\\dipEnv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\osorn\\anaconda3\\envs\\dipEnv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\osorn\\anaconda3\\envs\\dipEnv\\Lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:52\u001b[39m, in \u001b[36mDebertaV2SelfOutput.forward\u001b[39m\u001b[34m(self, hidden_states, input_tensor)\u001b[39m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states, input_tensor):\n\u001b[32m     51\u001b[39m     hidden_states = \u001b[38;5;28mself\u001b[39m.dense(hidden_states)\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m     hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     53\u001b[39m     hidden_states = \u001b[38;5;28mself\u001b[39m.LayerNorm(hidden_states + input_tensor)\n\u001b[32m     54\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\osorn\\anaconda3\\envs\\dipEnv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\osorn\\anaconda3\\envs\\dipEnv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\osorn\\anaconda3\\envs\\dipEnv\\Lib\\site-packages\\torch\\nn\\modules\\dropout.py:73\u001b[39m, in \u001b[36mDropout.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m     70\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[33;03m    Runs the forward pass.\u001b[39;00m\n\u001b[32m     72\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\osorn\\anaconda3\\envs\\dipEnv\\Lib\\site-packages\\torch\\nn\\functional.py:1418\u001b[39m, in \u001b[36mdropout\u001b[39m\u001b[34m(input, p, training, inplace)\u001b[39m\n\u001b[32m   1415\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m p < \u001b[32m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p > \u001b[32m1.0\u001b[39m:\n\u001b[32m   1416\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   1417\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m-> \u001b[39m\u001b[32m1418\u001b[39m     _VF.dropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1419\u001b[39m )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# ENTRENAR CON STRATIFIED K-FOLD (5 FOLDS)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"🚀 Iniciando entrenamiento con Stratified K-Fold...\")\n",
    "print(f\"⚙️  Configuración:\")\n",
    "print(f\"   Modelo: {config.MODEL_NAME}\")\n",
    "print(f\"   Folds: {config.N_FOLDS}\")\n",
    "print(f\"   Epochs: {config.NUM_EPOCHS}\")\n",
    "print(f\"   Batch size: {config.BATCH_SIZE}\")\n",
    "print(f\"   Learning rate: {config.LEARNING_RATE}\")\n",
    "print(f\"   Focal Loss γ: {config.FOCAL_GAMMA}\")\n",
    "print(f\"   R-Drop α: {config.R_DROP_ALPHA}\")\n",
    "print(f\"   Label smoothing: {config.LABEL_SMOOTHING}\")\n",
    "print(f\"   Early stopping patience: {config.EARLY_STOPPING_PATIENCE}\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# NOTA: Este entrenamiento puede tomar VARIAS HORAS en CPU\n",
    "# Cada fold con 6 épocas puede tomar ~1-2 horas\n",
    "# Total estimado: 5-10 horas para 5 folds\n",
    "\n",
    "fold_results, best_models = stratified_kfold_training(\n",
    "    train_texts, \n",
    "    train_labels, \n",
    "    config\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"✅ Entrenamiento K-Fold completado!\")\n",
    "print(\"\\n📊 Resumen de resultados por fold:\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for i, results in enumerate(fold_results):\n",
    "    print(f\"Fold {i+1}:\")\n",
    "    print(f\"  F1 Macro:    {results['eval_f1_macro']:.4f}\")\n",
    "    print(f\"  F1 Weighted: {results['eval_f1_weighted']:.4f}\")\n",
    "    print(f\"  Accuracy:    {results['eval_accuracy']:.4f}\")\n",
    "\n",
    "# Promedios\n",
    "avg_f1_macro = np.mean([r['eval_f1_macro'] for r in fold_results])\n",
    "avg_f1_weighted = np.mean([r['eval_f1_weighted'] for r in fold_results])\n",
    "avg_accuracy = np.mean([r['eval_accuracy'] for r in fold_results])\n",
    "std_f1_macro = np.std([r['eval_f1_macro'] for r in fold_results])\n",
    "\n",
    "print(\"-\" * 70)\n",
    "print(f\"📈 Promedios ({config.N_FOLDS} folds):\")\n",
    "print(f\"  F1 Macro:    {avg_f1_macro:.4f} ± {std_f1_macro:.4f}\")\n",
    "print(f\"  F1 Weighted: {avg_f1_weighted:.4f}\")\n",
    "print(f\"  Accuracy:    {avg_accuracy:.4f}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2eb157",
   "metadata": {},
   "source": [
    "## 🚀 Entrenamiento con Stratified K-Fold\n",
    "\n",
    "Entrenar DeBERTa-v3-base con Focal Loss, R-Drop y 5-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17bc327e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Cargando dataset poem_sentiment...\n",
      "✅ Datos cargados:\n",
      "   Train: (892, 3)\n",
      "   Validation: (105, 3)\n",
      "   Test: (104, 3)\n",
      "\n",
      "🔧 Preprocesando datos...\n",
      "   Train: 892 → 843 (excluidos 49 mixed)\n",
      "   Validation: 105 → 105 (excluidos 0 mixed)\n",
      "   Test: 104 → 104 (excluidos 0 mixed)\n",
      "\n",
      "🗑️  Eliminando duplicados...\n",
      "   Eliminados 0 versos duplicados\n",
      "   Eliminados 0 versos duplicados\n",
      "   Eliminados 0 versos duplicados\n",
      "\n",
      "✨ Normalizando texto (preservando matices poéticos)...\n",
      "\n",
      "✅ Datos finales:\n",
      "   Train+Validation (para K-Fold): 948\n",
      "   Test: 104\n",
      "\n",
      "📊 Distribución de clases en Train+Validation:\n",
      "   Clase 0:   174 (18.35%)\n",
      "   Clase 1:   150 (15.82%)\n",
      "   Clase 2:   624 (65.82%)\n",
      "\n",
      "🎯 Datos listos para entrenamiento con K-Fold\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CARGAR Y PREPARAR DATOS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"📥 Cargando dataset poem_sentiment...\")\n",
    "\n",
    "splits = {\n",
    "    \"train\": \"data/train-00000-of-00001.parquet\",\n",
    "    \"validation\": \"data/validation-00000-of-00001.parquet\",\n",
    "    \"test\": \"data/test-00000-of-00001.parquet\",\n",
    "}\n",
    "base_uri = \"hf://datasets/google-research-datasets/poem_sentiment/\"\n",
    "parquet_engine = \"fastparquet\"\n",
    "\n",
    "df_train = pd.read_parquet(base_uri + splits[\"train\"], engine=parquet_engine)\n",
    "df_validation = pd.read_parquet(base_uri + splits[\"validation\"], engine=parquet_engine)\n",
    "df_test = pd.read_parquet(base_uri + splits[\"test\"], engine=parquet_engine)\n",
    "\n",
    "print(f\"✅ Datos cargados:\")\n",
    "print(f\"   Train: {df_train.shape}\")\n",
    "print(f\"   Validation: {df_validation.shape}\")\n",
    "print(f\"   Test: {df_test.shape}\")\n",
    "\n",
    "# ============================================================================\n",
    "# PREPROCESAMIENTO: EXCLUIR CLASE 3 (MIXED)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n🔧 Preprocesando datos...\")\n",
    "\n",
    "# Excluir clase 3 (mixed) de todos los splits\n",
    "df_train_clean = df_train[df_train['label'] != 3].copy()\n",
    "df_validation_clean = df_validation[df_validation['label'] != 3].copy()\n",
    "df_test_clean = df_test[df_test['label'] != 3].copy()\n",
    "\n",
    "print(f\"   Train: {len(df_train)} → {len(df_train_clean)} (excluidos {len(df_train) - len(df_train_clean)} mixed)\")\n",
    "print(f\"   Validation: {len(df_validation)} → {len(df_validation_clean)} (excluidos {len(df_validation) - len(df_validation_clean)} mixed)\")\n",
    "print(f\"   Test: {len(df_test)} → {len(df_test_clean)} (excluidos {len(df_test) - len(df_test_clean)} mixed)\")\n",
    "\n",
    "# Eliminar duplicados\n",
    "print(\"\\n🗑️  Eliminando duplicados...\")\n",
    "df_train_clean = remove_duplicate_verses(df_train_clean)\n",
    "df_validation_clean = remove_duplicate_verses(df_validation_clean)\n",
    "df_test_clean = remove_duplicate_verses(df_test_clean)\n",
    "\n",
    "# Normalizar texto (sin stemming/stopwords)\n",
    "print(\"\\n✨ Normalizando texto (preservando matices poéticos)...\")\n",
    "df_train_clean['verse_text'] = df_train_clean['verse_text'].apply(advanced_text_preprocessing)\n",
    "df_validation_clean['verse_text'] = df_validation_clean['verse_text'].apply(advanced_text_preprocessing)\n",
    "df_test_clean['verse_text'] = df_test_clean['verse_text'].apply(advanced_text_preprocessing)\n",
    "\n",
    "# Combinar train + validation para K-Fold\n",
    "df_train_full = pd.concat([df_train_clean, df_validation_clean], ignore_index=True)\n",
    "\n",
    "print(f\"\\n✅ Datos finales:\")\n",
    "print(f\"   Train+Validation (para K-Fold): {len(df_train_full)}\")\n",
    "print(f\"   Test: {len(df_test_clean)}\")\n",
    "\n",
    "# Distribución de clases\n",
    "print(f\"\\n📊 Distribución de clases en Train+Validation:\")\n",
    "for label, count in df_train_full['label'].value_counts().sort_index().items():\n",
    "    percentage = count / len(df_train_full) * 100\n",
    "    print(f\"   Clase {label}: {count:5d} ({percentage:5.2f}%)\")\n",
    "\n",
    "# Preparar datos para entrenamiento\n",
    "train_texts = df_train_full['verse_text'].tolist()\n",
    "train_labels = df_train_full['label'].tolist()\n",
    "\n",
    "test_texts = df_test_clean['verse_text'].tolist()\n",
    "test_labels = df_test_clean['label'].tolist()\n",
    "\n",
    "print(f\"\\n🎯 Datos listos para entrenamiento con K-Fold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd968b0",
   "metadata": {},
   "source": [
    "## 📊 Carga y Preprocesamiento de Datos\n",
    "\n",
    "Cargar dataset, eliminar clase 3 (mixed), duplicados y normalizar texto."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dipEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
