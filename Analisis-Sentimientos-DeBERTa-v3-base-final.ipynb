{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2dd9636a",
   "metadata": {},
   "source": [
    "## 7) Discusión y conclusiones\n",
    "\n",
    "Observaciones principales:\n",
    "- El desbalance (≈66% `no_impact`) induce sesgos hacia la clase mayoritaria; Focal Loss + class weights elevaron F1‑macro y balancearon el aprendizaje.\n",
    "- La clase `positive` fue la más desafiante (F1≈0.81 en Test), con confusiones hacia `no_impact` cuando el verso es más descriptivo que valorativo.\n",
    "- La clase `negative` logró alta recall (≈1.00), señal de que sus marcadores son más explícitos.\n",
    "\n",
    "¿Fue exitoso el entrenamiento?\n",
    "- Sí. F1‑macro Test = 0.8909 (> 0.85). Curvas por época estables con early stopping; label smoothing ayudó a la calibración.\n",
    "- `max_len=128` fue suficiente para la mayoría de versos.\n",
    "\n",
    "Escenarios de uso:\n",
    "- Curaduría y análisis de sentimiento en colecciones poéticas.\n",
    "- Herramientas de apoyo editorial y analítica creativa.\n",
    "- Investigación de marcadores afectivos en poesía.\n",
    "\n",
    "Limitaciones y mejoras:\n",
    "- Mejorar la clase `positive`: augmentations suaves, más épocas con early stopping, o ensembles de seeds/épocas.\n",
    "- Probar modelos alternativos (DeBERTa‑v3‑large, RoBERTa‑large) si hay GPU; distilados/quantization para despliegue eficiente.\n",
    "- Reincorporar `mixed` requerirá más datos o ajuste de pérdidas/umbral.\n",
    "\n",
    "Reproducibilidad y despliegue:\n",
    "- Fijar seeds y registrar versiones de dependencias.\n",
    "- Exportar checkpoint + tokenizer; opcionalmente un endpoint (FastAPI/Gradio) para demo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57c1432",
   "metadata": {},
   "source": [
    "## 6) Ejemplos de uso (inferencia)\n",
    "\n",
    "Ejemplos de entradas y etiquetas esperadas:\n",
    "- “The night is dark and full of fears.” → `negative`\n",
    "- “Sunshine fills my heart with joy.” → `positive`\n",
    "- “The sky is blue and the grass is green.” → `no_impact`\n",
    "\n",
    "Cómo aplicar (resumen):\n",
    "1) Tokenizar el verso con el tokenizer del checkpoint final.\n",
    "2) Generar tensores (máx. longitud 128) y pasarlos al modelo.\n",
    "3) Tomar `argmax` de los logits y mapear a `{0: negative, 1: positive, 2: no_impact}`.\n",
    "\n",
    "Nota: si se dispone del checkpoint guardado, puede añadirse una celda de “demo de inferencia” para procesar una lista de versos de ejemplo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c292a79",
   "metadata": {},
   "source": [
    "## 5) Evaluación: F1‑macro y matriz de confusión\n",
    "\n",
    "Resultados obtenidos en Colab (Test):\n",
    "- Accuracy: 0.9135\n",
    "- F1‑Macro: 0.8909\n",
    "- F1‑Weighted: 0.9136\n",
    "\n",
    "Tabla comparativa (baselines marcados como pendientes):\n",
    "\n",
    "| Modelo                         | Features                 | F1‑macro (Val) | F1‑macro (Test) | Accuracy (Test) | Notas                                  |\n",
    "|--------------------------------|--------------------------|----------------|-----------------|-----------------|----------------------------------------|\n",
    "| TF‑IDF + LinearSVC             | n‑gramas 1–2/1–3         | Pendiente      | Pendiente       | Pendiente       | Baseline clásico rápido                 |\n",
    "| TextCNN/BiLSTM                 | Embeddings + CNN/LSTM    | Pendiente      | Pendiente       | Pendiente       | Baseline neural ligero                  |\n",
    "| DeBERTa‑v3‑base (fine‑tuning)  | Subword tokenizer        | 0.8903         | 0.8909          | 0.9135          | Focal Loss + weights; label smoothing   |\n",
    "\n",
    "Matriz de confusión (Test) y observaciones:\n",
    "- Negative: precision 0.86, recall 1.00 → casi sin falsos negativos.\n",
    "- Positive: precision 0.81, recall 0.81 → clase más desafiante; confusiones con `no_impact`.\n",
    "- No_impact: precision 0.95, recall 0.91 → muy bien capturada, consistente con su mayoría.\n",
    "\n",
    "Requisito: al menos un modelo con F1‑macro ≥ 0.85 → Cumplido por DeBERTa‑v3‑base.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902fbe62",
   "metadata": {},
   "source": [
    "## 4) Uso de arquitecturas preentrenadas\n",
    "\n",
    "- Se empleó un transformador preentrenado (`microsoft/deberta‑v3‑base`) con ajuste fino completo en el dataset objetivo.\n",
    "- Se cumple el requisito de que al menos una arquitectura sea entrenada/ajustada finamente.\n",
    "- Alternativas rápidas para comparación: DistilBERT, BERT base, RoBERTa base (menor costo que DeBERTa, aunque típicamente menor desempeño en este dominio).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e891c0b",
   "metadata": {},
   "source": [
    "## 3) Arquitecturas propuestas\n",
    "\n",
    "A) Clásica: TF‑IDF + Linear SVM (LinearSVC)\n",
    "- Pipeline: limpieza ligera → TF‑IDF (n‑gramas 1–2/1–3) → LinearSVC.\n",
    "- Ventajas: rápido, baseline fuerte en textos cortos; robusto con `class_weight='balanced'`.\n",
    "- Desventajas: no captura contexto largo ni semántica contextual.\n",
    "- Hiperparámetros recomendados: `max_features=50k–100k`, `ngram_range=(1,2)` o `(1,3)`, `C∈[0.5,2.0]`.\n",
    "\n",
    "B) Neural “ligero”: TextCNN o BiLSTM\n",
    "- Pipeline: tokenización simple → embeddings (preentrenados o entrenables) → CNN 1D (filtros 3/4/5) o BiLSTM → densa.\n",
    "- Ventajas: CNN capta patrones locales; BiLSTM capta dependencias secuenciales.\n",
    "- Desventajas: se entrena desde cero; sensible a regularización y early stopping.\n",
    "- Sugerencias: `max_len=128`, `emb_dim=100–300`, `dropout=0.3–0.5`, `Adam lr≈1e‑3`.\n",
    "\n",
    "C) Transformador preentrenado: DeBERTa‑v3‑base (fine‑tuning)\n",
    "- Técnica: Focal Loss (γ=1.5) con class weights, label smoothing=0.1, early stopping.\n",
    "- Ventajas: captura semántica y contexto; resultados SOTA.\n",
    "- Desventajas: requiere GPU para tiempos razonables; mayor memoria.\n",
    "- Hiperparámetros usados: `batch_size=16`, `lr=2e‑5`, `epochs=6`, `max_len=128`, `warmup_ratio=0.05`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea68c5a",
   "metadata": {},
   "source": [
    "## 2) Preprocesamiento y justificación\n",
    "\n",
    "Preprocesamiento aplicado (ligero y no destructivo):\n",
    "- Normalización de comillas y guiones: unifica variantes Unicode a ASCII para evitar tokens raros.\n",
    "- Colapsado de espacios múltiples y recorte (trim).\n",
    "- Eliminación de versos duplicados por coincidencia exacta dentro de cada split.\n",
    "\n",
    "Decisiones explícitas:\n",
    "- Sin stemming ni stopwords: en poesía, la morfología y las llamadas “stopwords” aportan matices; removerlas suele degradar señales sutiles de sentimiento.\n",
    "- Sin lowercasing forzado: los tokenizadores subword (p. ej., DeBERTa) manejan mayúsculas; además, en poesía las mayúsculas pueden portar información estilística.\n",
    "\n",
    "Impacto esperado:\n",
    "- Reducir ruido tipográfico sin perder semántica ni estilo del verso.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e050a2f0",
   "metadata": {},
   "source": [
    "## 1) Importar el conjunto de datos\n",
    "\n",
    "- Fuente: Hugging Face Dataset `google-research-datasets/poem_sentiment` en formato Parquet accesible vía URIs `hf://`.\n",
    "- Splits usados: `train`, `validation` y `test`.\n",
    "- Exclusión: se excluye la clase `3` (mixed) en todos los splits para formular un problema de 3 clases: `0 = negative`, `1 = positive`, `2 = no_impact`.\n",
    "- Tamaños tras la exclusión (según la corrida en Colab):\n",
    "  - Train: 843\n",
    "  - Validation: 105\n",
    "  - Test: 104\n",
    "- Distribución por clase (aprox.): `no_impact ~66%`, `negative ~18%`, `positive ~16%` → dataset desbalanceado.\n",
    "\n",
    "Justificación de la fuente y formato:\n",
    "- El acceso vía `hf://` evita descargas manuales y favorece reproducibilidad.\n",
    "- Parquet es eficiente y se integra bien con `fastparquet`/`pyarrow` para lectura rápida.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5463409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fastparquet in c:\\users\\osorn\\anaconda3\\envs\\dipenv\\lib\\site-packages (2024.11.0)\n",
      "Requirement already satisfied: pandas>=1.5.0 in c:\\users\\osorn\\anaconda3\\envs\\dipenv\\lib\\site-packages (from fastparquet) (2.3.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\osorn\\anaconda3\\envs\\dipenv\\lib\\site-packages (from fastparquet) (2.3.4)\n",
      "Requirement already satisfied: cramjam>=2.3 in c:\\users\\osorn\\anaconda3\\envs\\dipenv\\lib\\site-packages (from fastparquet) (2.11.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\osorn\\anaconda3\\envs\\dipenv\\lib\\site-packages (from fastparquet) (2025.9.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\osorn\\anaconda3\\envs\\dipenv\\lib\\site-packages (from fastparquet) (25.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\osorn\\anaconda3\\envs\\dipenv\\lib\\site-packages (from pandas>=1.5.0->fastparquet) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\osorn\\anaconda3\\envs\\dipenv\\lib\\site-packages (from pandas>=1.5.0->fastparquet) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\osorn\\anaconda3\\envs\\dipenv\\lib\\site-packages (from pandas>=1.5.0->fastparquet) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\osorn\\anaconda3\\envs\\dipenv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.5.0->fastparquet) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install fastparquet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2246c3ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Model: microsoft/deberta-v3-base\n",
      "Max length: 128\n",
      "Batch size: 16\n",
      "Learning rate: 2e-05\n",
      "\n",
      "✅ Configuración avanzada completada\n",
      "🚀 Listo para implementar DeBERTa-v3-base con técnicas de regularización avanzadas\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# IMPORTS Y CONFIGURACIÓN INICIAL - DEBERTA-V3-BASE PARA POEM_SENTIMENT\n",
    "# ============================================================================\n",
    "\n",
    "# Instala dependencias avanzadas\n",
    "#%pip install transformers torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "#%pip install datasets accelerate peft\n",
    "#%pip install scikit-learn numpy pandas matplotlib seaborn\n",
    "#%pip install nltk fastparquet\n",
    "\n",
    "# ============================================================================\n",
    "# IMPORTS DE LIBRERÍAS\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, LinearLR\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    TrainingArguments, Trainer, EarlyStoppingCallback,\n",
    "    DataCollatorWithPadding, set_seed\n",
    ")\n",
    "from datasets import Dataset as HFDataset\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Desactivar warnings molestos\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "set_seed(42)\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURACIÓN GLOBAL\n",
    "# ============================================================================\n",
    "\n",
    "class Config:\n",
    "    # Modelo\n",
    "    MODEL_NAME = \"microsoft/deberta-v3-base\"\n",
    "    MAX_LEN = 128\n",
    "    NUM_LABELS = 3  # Excluyendo clase 3 (mixed)\n",
    "\n",
    "    # Entrenamiento\n",
    "    BATCH_SIZE = 16\n",
    "    LEARNING_RATE = 2e-5\n",
    "    NUM_EPOCHS = 6\n",
    "    WARMUP_RATIO = 0.05\n",
    "    WEIGHT_DECAY = 0.01\n",
    "\n",
    "    # Regularización\n",
    "    LABEL_SMOOTHING = 0.1\n",
    "    FOCAL_GAMMA = 1.5\n",
    "    R_DROP_ALPHA = 4.0\n",
    "\n",
    "    # Validación\n",
    "    N_FOLDS = 5\n",
    "    EARLY_STOPPING_PATIENCE = 3\n",
    "\n",
    "    # Paths\n",
    "    OUTPUT_DIR = \"./deberta_results\"\n",
    "    LOGGING_DIR = \"./logs\"\n",
    "\n",
    "    # Device\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "config = Config()\n",
    "\n",
    "print(f\"Using device: {config.DEVICE}\")\n",
    "print(f\"Model: {config.MODEL_NAME}\")\n",
    "print(f\"Max length: {config.MAX_LEN}\")\n",
    "print(f\"Batch size: {config.BATCH_SIZE}\")\n",
    "print(f\"Learning rate: {config.LEARNING_RATE}\")\n",
    "\n",
    "# ============================================================================\n",
    "# PREPROCESAMIENTO AVANZADO\n",
    "# ============================================================================\n",
    "\n",
    "def advanced_text_preprocessing(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Preprocesamiento avanzado para texto poético:\n",
    "    - Eliminar duplicados de versos\n",
    "    - Normalizar comillas y guiones\n",
    "    - NO stemming/stopwords (preservar matices)\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    # Normalizar comillas (usando unicode characters)\n",
    "    text = re.sub(r'[“”«»„\"]', '\"', text) # Double quotes\n",
    "    text = re.sub(r'[‘’‹›`´]', \"'\", text) # Single quotes and backticks/accents\n",
    "\n",
    "    # Normalizar guiones\n",
    "    text = re.sub(r'[–—]', '-', text)\n",
    "\n",
    "    # Eliminar espacios múltiples\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    # Eliminar espacios al inicio/fin\n",
    "    text = text.strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "def remove_duplicate_verses(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Eliminar versos duplicados dentro del mismo dataset.\"\"\"\n",
    "    initial_len = len(df)\n",
    "    df_clean = df.drop_duplicates(subset=['verse_text'], keep='first')\n",
    "    final_len = len(df_clean)\n",
    "\n",
    "    print(f\"Removed {initial_len - final_len} duplicate verses\")\n",
    "    return df_clean\n",
    "\n",
    "# ============================================================================\n",
    "# FOCAL LOSS PARA DESBALANCE\n",
    "# ============================================================================\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"Focal Loss para manejar desbalance de clases.\"\"\"\n",
    "    def __init__(self, alpha=None, gamma=config.FOCAL_GAMMA, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none', weight=self.alpha)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = ((1 - pt) ** self.gamma) * ce_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "# ============================================================================\n",
    "# R-DROP REGULARIZATION\n",
    "# ============================================================================\n",
    "\n",
    "def r_drop_loss(logits1, logits2, targets, alpha=config.R_DROP_ALPHA):\n",
    "    \"\"\"R-Drop regularization para mejorar generalización.\"\"\"\n",
    "    loss1 = F.cross_entropy(logits1, targets, reduction='mean')\n",
    "    loss2 = F.cross_entropy(logits2, targets, reduction='mean')\n",
    "\n",
    "    # KL divergence entre las dos predicciones\n",
    "    kl_loss = F.kl_div(\n",
    "        F.log_softmax(logits1, dim=-1),\n",
    "        F.softmax(logits2, dim=-1),\n",
    "        reduction='batchmean'\n",
    "    )\n",
    "\n",
    "    return (loss1 + loss2) / 2 + alpha * kl_loss\n",
    "\n",
    "# ============================================================================\n",
    "# DATASET CUSTOM PARA HUGGINGFACE\n",
    "# ============================================================================\n",
    "\n",
    "class PoemSentimentDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=config.MAX_LEN):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# ============================================================================\n",
    "# CUSTOM TRAINER CON FOCAL LOSS Y R-DROP\n",
    "# ============================================================================\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, class_weights=None, use_r_drop=False, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = class_weights\n",
    "        self.use_r_drop = use_r_drop\n",
    "\n",
    "        if class_weights is not None:\n",
    "            self.criterion = FocalLoss(alpha=class_weights.to(self.model.device))\n",
    "        else:\n",
    "            self.criterion = FocalLoss()\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "\n",
    "        if self.use_r_drop:\n",
    "            # Forward pass 1\n",
    "            outputs1 = model(**inputs)\n",
    "            logits1 = outputs1.logits\n",
    "\n",
    "            # Forward pass 2 (con dropout diferente)\n",
    "            outputs2 = model(**inputs)\n",
    "            logits2 = outputs2.logits\n",
    "\n",
    "            # R-Drop loss\n",
    "            loss = r_drop_loss(logits1, logits2, labels)\n",
    "        else:\n",
    "            # Forward pass normal\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            loss = self.criterion(logits, labels)\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# ============================================================================\n",
    "# MÉTRICAS CUSTOM PARA F1 MACRO\n",
    "# ============================================================================\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    f1_macro = f1_score(labels, predictions, average='macro')\n",
    "    f1_weighted = f1_score(labels, predictions, average='weighted')\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1_macro': f1_macro,\n",
    "        'f1_weighted': f1_weighted\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# VALIDACIÓN CON STRATIFIED K-FOLD\n",
    "# ============================================================================\n",
    "\n",
    "def stratified_kfold_training(train_texts, train_labels, config):\n",
    "    \"\"\"Entrenamiento con Stratified K-Fold para robustez.\"\"\"\n",
    "\n",
    "    # Configurar estratificación\n",
    "    skf = StratifiedKFold(n_splits=config.N_FOLDS, shuffle=True, random_state=42)\n",
    "\n",
    "    fold_results = []\n",
    "    best_models = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(train_texts, train_labels)):\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"FOLD {fold + 1}/{config.N_FOLDS}\")\n",
    "        print(f\"{'='*50}\")\n",
    "\n",
    "        # Split data\n",
    "        fold_train_texts = [train_texts[i] for i in train_idx]\n",
    "        fold_train_labels = [train_labels[i] for i in train_idx]\n",
    "        fold_val_texts = [train_texts[i] for i in val_idx]\n",
    "        fold_val_labels = [train_labels[i] for i in val_idx]\n",
    "\n",
    "        # Calcular class weights para este fold\n",
    "        class_weights = compute_class_weight(\n",
    "            'balanced',\n",
    "            classes=np.unique(fold_train_labels),\n",
    "            y=fold_train_labels\n",
    "        )\n",
    "        class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
    "\n",
    "        # Tokenizer y modelo\n",
    "        tokenizer = AutoTokenizer.from_pretrained(config.MODEL_NAME)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            config.MODEL_NAME,\n",
    "            num_labels=config.NUM_LABELS,\n",
    "            label_smoothing=config.LABEL_SMOOTHING\n",
    "        )\n",
    "\n",
    "        # Datasets\n",
    "        train_dataset = PoemSentimentDataset(fold_train_texts, fold_train_labels, tokenizer)\n",
    "        val_dataset = PoemSentimentDataset(fold_val_texts, fold_val_labels, tokenizer)\n",
    "\n",
    "        # Training arguments\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=f\"{config.OUTPUT_DIR}/fold_{fold}\",\n",
    "            logging_dir=f\"{config.LOGGING_DIR}/fold_{fold}\",\n",
    "            num_train_epochs=config.NUM_EPOCHS,\n",
    "            per_device_train_batch_size=config.BATCH_SIZE,\n",
    "            per_device_eval_batch_size=config.BATCH_SIZE * 2,\n",
    "            learning_rate=config.LEARNING_RATE,\n",
    "            weight_decay=config.WEIGHT_DECAY,\n",
    "            warmup_ratio=config.WARMUP_RATIO,\n",
    "            lr_scheduler_type=\"cosine\",\n",
    "            save_strategy=\"epoch\",\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"f1_macro\",\n",
    "            greater_is_better=True,\n",
    "            save_total_limit=2,\n",
    "            logging_steps=50,\n",
    "            report_to=\"none\"\n",
    "        )\n",
    "\n",
    "        # Callbacks\n",
    "        early_stopping = EarlyStoppingCallback(\n",
    "            early_stopping_patience=config.EARLY_STOPPING_PATIENCE\n",
    "        )\n",
    "\n",
    "        # Trainer\n",
    "        trainer = CustomTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            tokenizer=tokenizer,\n",
    "            compute_metrics=compute_metrics,\n",
    "            callbacks=[early_stopping],\n",
    "            class_weights=class_weights,\n",
    "            use_r_drop=True\n",
    "        )\n",
    "\n",
    "        # Entrenar\n",
    "        trainer.train()\n",
    "\n",
    "        # Evaluar en validation\n",
    "        eval_results = trainer.evaluate()\n",
    "        fold_results.append(eval_results)\n",
    "\n",
    "        # Guardar mejor modelo de este fold\n",
    "        best_models.append(trainer.model)\n",
    "\n",
    "        print(f\"Fold {fold + 1} - F1 Macro: {eval_results['eval_f1_macro']:.4f}\")\n",
    "\n",
    "    return fold_results, best_models\n",
    "\n",
    "# ============================================================================\n",
    "# ENSEMBLE DE CHECKPOINTS\n",
    "# ============================================================================\n",
    "\n",
    "def ensemble_predictions(models, tokenizer, test_texts, config):\n",
    "    \"\"\"Ensamblar predicciones de múltiples modelos.\"\"\"\n",
    "    all_predictions = []\n",
    "\n",
    "    test_dataset = PoemSentimentDataset(test_texts, [0]*len(test_texts), tokenizer)\n",
    "\n",
    "    for model in models:\n",
    "        model.eval()\n",
    "        predictions = []\n",
    "\n",
    "        dataloader = DataLoader(\n",
    "            test_dataset,\n",
    "            batch_size=config.BATCH_SIZE * 4,\n",
    "            shuffle=False\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                input_ids = batch['input_ids'].to(config.DEVICE)\n",
    "                attention_mask = batch['attention_mask'].to(config.DEVICE)\n",
    "\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                logits = outputs.logits\n",
    "                predictions.extend(logits.cpu().numpy())\n",
    "\n",
    "        all_predictions.append(np.array(predictions))\n",
    "\n",
    "    # Promediar logits\n",
    "    ensemble_logits = np.mean(all_predictions, axis=0)\n",
    "    ensemble_predictions = np.argmax(ensemble_logits, axis=1)\n",
    "\n",
    "    return ensemble_predictions, ensemble_logits\n",
    "\n",
    "print(\"\\n✅ Configuración avanzada completada\")\n",
    "print(\"🚀 Listo para implementar DeBERTa-v3-base con técnicas de regularización avanzadas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612b7321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# IMPLEMENTACIÓN COMPLETA: DEBERTA-V3-BASE PARA POEM_SENTIMENT\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"🚀 IMPLEMENTACIÓN DEBERTA-V3-BASE PARA POEM_SENTIMENT\")\n",
    "print(\"=\" + \"=\" * 80)\n",
    "\n",
    "# ============================================================================\n",
    "# 1. CARGA Y PREPROCESAMIENTO DE DATOS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n1️⃣ CARGANDO Y PREPROCESANDO DATOS...\")\n",
    "\n",
    "# Cargar datos\n",
    "splits = {\n",
    "    \"train\": \"data/train-00000-of-00001.parquet\",\n",
    "    \"validation\": \"data/validation-00000-of-00001.parquet\",\n",
    "    \"test\": \"data/test-00000-of-00001.parquet\",\n",
    "}\n",
    "base_uri = \"hf://datasets/google-research-datasets/poem_sentiment/\"\n",
    "parquet_engine = \"fastparquet\"\n",
    "\n",
    "df_train = pd.read_parquet(base_uri + splits[\"train\"], engine=parquet_engine)\n",
    "df_validation = pd.read_parquet(base_uri + splits[\"validation\"], engine=parquet_engine)\n",
    "df_test = pd.read_parquet(base_uri + splits[\"test\"], engine=parquet_engine)\n",
    "\n",
    "print(f\"Datos originales:\")\n",
    "print(f\"  Train: {len(df_train)} muestras\")\n",
    "print(f\"  Validation: {len(df_validation)} muestras\")\n",
    "print(f\"  Test: {len(df_test)} muestras\")\n",
    "\n",
    "# Preprocesamiento avanzado\n",
    "print(\"\\n🔧 Aplicando preprocesamiento avanzado...\")\n",
    "\n",
    "def advanced_text_preprocessing(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Preprocesamiento avanzado para texto poético:\n",
    "    - Normalizar comillas y guiones\n",
    "    - Eliminar espacios múltiples\n",
    "    - Eliminar espacios al inicio/fin\n",
    "    - NO stemming/stopwords (preservar matices)\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    # Normalizar comillas (usando unicode characters)\n",
    "    text = re.sub(r'[“”«»„\"]', '\"', text) # Double quotes\n",
    "    text = re.sub(r'[‘’‹›`´]', \"'\", text) # Single quotes and backticks/accents\n",
    "\n",
    "    # Normalizar guiones\n",
    "    text = re.sub(r'[–—]', '-', text)\n",
    "\n",
    "    # Eliminar espacios múltiples\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    # Eliminar espacios al inicio/fin\n",
    "    text = text.strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "def remove_duplicate_verses(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Eliminar versos duplicados dentro del mismo dataset.\"\"\"\n",
    "    initial_len = len(df)\n",
    "    df_clean = df.drop_duplicates(subset=['verse_text'], keep='first')\n",
    "    final_len = len(df_clean)\n",
    "    if initial_len - final_len > 0:\n",
    "        print(f\"Removed {initial_len - final_len} duplicate verses\")\n",
    "    return df_clean\n",
    "\n",
    "\n",
    "# Eliminar duplicados\n",
    "df_train = remove_duplicate_verses(df_train)\n",
    "df_validation = remove_duplicate_verses(df_validation)\n",
    "df_test = remove_duplicate_verses(df_test)\n",
    "\n",
    "# Aplicar preprocesamiento de texto\n",
    "df_train['verse_text'] = df_train['verse_text'].apply(advanced_text_preprocessing)\n",
    "df_validation['verse_text'] = df_validation['verse_text'].apply(advanced_text_preprocessing)\n",
    "df_test['verse_text'] = df_test['verse_text'].apply(advanced_text_preprocessing)\n",
    "\n",
    "# EXCLUIR CLASE 3 (mixed) COMPLETAMENTE\n",
    "print(\"\\n❌ Excluyendo clase 'mixed' (3) de todos los splits...\")\n",
    "mask_train = df_train['label'] != 3\n",
    "mask_val = df_validation['label'] != 3\n",
    "mask_test = df_test['label'] != 3\n",
    "\n",
    "df_train = df_train[mask_train].reset_index(drop=True)\n",
    "df_validation = df_validation[mask_val].reset_index(drop=True)\n",
    "df_test = df_test[mask_test].reset_index(drop=True)\n",
    "\n",
    "print(f\"Datos después de excluir clase 3:\")\n",
    "print(f\"  Train: {len(df_train)} muestras\")\n",
    "print(f\"  Validation: {len(df_validation)} muestras\")\n",
    "print(f\"  Test: {len(df_test)} muestras\")\n",
    "\n",
    "# Preparar arrays finales\n",
    "train_texts = df_train['verse_text'].tolist()\n",
    "train_labels = df_train['label'].values\n",
    "\n",
    "val_texts = df_validation['verse_text'].tolist()\n",
    "val_labels = df_validation['label'].values\n",
    "\n",
    "test_texts = df_test['verse_text'].tolist()\n",
    "test_labels = df_test['label'].values\n",
    "\n",
    "# Verificar distribución final\n",
    "print(\"\\n📊 Distribución final de clases:\")\n",
    "for name, labels in [(\"Train\", train_labels), (\"Validation\", val_labels), (\"Test\", test_labels)]:\n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    print(f\"  {name}: \", end=\"\")\n",
    "    for label, count in zip(unique, counts):\n",
    "        class_name = {0: 'negative', 1: 'positive', 2: 'no_impact'}.get(label, f'class_{label}')\n",
    "        pct = 100 * count / len(labels)\n",
    "        print(f\"{class_name}={count}({pct:.1f}%) \", end=\"\")\n",
    "    print()\n",
    "\n",
    "# ============================================================================\n",
    "# 2. TOKENIZACIÓN Y DATASET PREPARACIÓN (HUGGING FACE)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n2️⃣ TOKENIZANDO Y PREPARANDO DATASETS (Hugging Face)...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.MODEL_NAME)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=config.MAX_LEN)\n",
    "\n",
    "# Convertir a Hugging Face Datasets\n",
    "train_hf = HFDataset.from_pandas(pd.DataFrame({'text': train_texts, 'label': train_labels}))\n",
    "val_hf = HFDataset.from_pandas(pd.DataFrame({'text': val_texts, 'label': val_labels}))\n",
    "test_hf = HFDataset.from_pandas(pd.DataFrame({'text': test_texts, 'label': test_labels}))\n",
    "\n",
    "# Aplicar tokenización\n",
    "tokenized_train = train_hf.map(tokenize_function, batched=True)\n",
    "tokenized_val = val_hf.map(tokenize_function, batched=True)\n",
    "tokenized_test = test_hf.map(tokenize_function, batched=True)\n",
    "\n",
    "print(\"✅ Datasets tokenizados.\")\n",
    "print(tokenized_train)\n",
    "print(tokenized_val)\n",
    "print(tokenized_test)\n",
    "\n",
    "# ============================================================================\n",
    "# 3. ANÁLISIS DE DESBALANCE Y CLASS WEIGHTS (para Trainer)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n3️⃣ ANALIZANDO DESBALANCE Y CALCULANDO CLASS WEIGHTS...\")\n",
    "\n",
    "# Calcular class weights globales\n",
    "class_weights = compute_class_weight(\n",
    "    'balanced',\n",
    "    classes=np.unique(train_labels),\n",
    "    y=train_labels\n",
    ")\n",
    "\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(config.DEVICE)\n",
    "print(f\"Class weights (for Trainer): {class_weights_tensor.cpu().numpy()}\")\n",
    "print(f\"  • Negative (0): {class_weights_tensor[0].item():.3f}\")\n",
    "print(f\"  • Positive (1): {class_weights_tensor[1].item():.3f}\")\n",
    "print(f\"  • No_impact (2): {class_weights_tensor[2].item():.3f}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 4. CONFIGURACIÓN DEL MODELO Y TRAINER\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n4️⃣ CONFIGURANDO MODELO Y TRAINER...\")\n",
    "\n",
    "# Cargar modelo pre-entrenado\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    config.MODEL_NAME,\n",
    "    num_labels=config.NUM_LABELS\n",
    ")\n",
    "\n",
    "# Custom Trainer con Focal Loss\n",
    "class CustomTrainerWithFocalLoss(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Aplicar Focal Loss con class weights\n",
    "        criterion = FocalLoss(alpha=class_weights_tensor)\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# Define SEED\n",
    "SEED = 42\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=config.OUTPUT_DIR,\n",
    "    logging_dir=config.LOGGING_DIR,\n",
    "    num_train_epochs=config.NUM_EPOCHS,\n",
    "    per_device_train_batch_size=config.BATCH_SIZE,\n",
    "    per_device_eval_batch_size=config.BATCH_SIZE * 2,\n",
    "    learning_rate=config.LEARNING_RATE,\n",
    "    weight_decay=config.WEIGHT_DECAY,\n",
    "    warmup_ratio=config.WARMUP_RATIO,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\", # Corrected parameter name\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_macro\", # Optimizar por F1-macro\n",
    "    greater_is_better=True,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=50,\n",
    "    report_to=\"none\",\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStoppingCallback(\n",
    "    early_stopping_patience=config.EARLY_STOPPING_PATIENCE\n",
    ")\n",
    "\n",
    "# Inicializar Trainer\n",
    "trainer = CustomTrainerWithFocalLoss(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[early_stopping],\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer) # Usar DataCollator\n",
    ")\n",
    "\n",
    "print(\"✅ Modelo y Trainer configurados.\")\n",
    "\n",
    "# ============================================================================\n",
    "# 5. ENTRENAMIENTO DEL MODELO\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n5️⃣ ENTRENANDO MODELO...\")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(\"✅ Entrenamiento completado.\")\n",
    "\n",
    "# ============================================================================\n",
    "# 6. EVALUACIÓN FINAL EN TEST SET\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n6️⃣ EVALUACIÓN FINAL EN TEST SET...\")\n",
    "\n",
    "eval_val_results = trainer.evaluate(eval_dataset=tokenized_val)\n",
    "\n",
    "print(\"\\n🎯 RESULTADOS FINALES EN eval_val_results SET:\")\n",
    "print(f\"  Accuracy:    {eval_val_results['eval_accuracy']:.4f}\")\n",
    "print(f\"  F1-Macro:    {eval_val_results['eval_f1_macro']:.4f}\")\n",
    "print(f\"  F1-Weighted: {eval_val_results['eval_f1_weighted']:.4f}\")\n",
    "\n",
    "#\n",
    "\n",
    "print(\"\\n6️⃣ EVALUACIÓN FINAL EN TEST SET...\")\n",
    "\n",
    "eval_results = trainer.evaluate(eval_dataset=tokenized_test)\n",
    "\n",
    "print(\"\\n🎯 RESULTADOS FINALES EN TEST SET:\")\n",
    "print(f\"  Accuracy:    {eval_results['eval_accuracy']:.4f}\")\n",
    "print(f\"  F1-Macro:    {eval_results['eval_f1_macro']:.4f}\")\n",
    "print(f\"  F1-Weighted: {eval_results['eval_f1_weighted']:.4f}\")\n",
    "\n",
    "# Verificar objetivo\n",
    "target_f1 = 0.85\n",
    "achieved = eval_results['eval_f1_macro'] >= target_f1\n",
    "\n",
    "if achieved:\n",
    "    print(f\"\\n🎉🎉🎉 ¡OBJETIVO ALCANZADO! F1-Macro >= {target_f1}\")\n",
    "else:\n",
    "    gap = target_f1 - eval_results['eval_f1_macro']\n",
    "    print(f\"\\n⏳ OBJETIVO NO ALCANZADO. Gap: {gap:.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 7. ANÁLISIS DETALLADO Y VISUALIZACIONES (Test Set)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n7️⃣ ANÁLISIS DETALLADO Y VISUALIZACIONES (Test Set)...\")\n",
    "\n",
    "# Obtener predicciones en test\n",
    "predictions = trainer.predict(tokenized_test)\n",
    "test_preds = np.argmax(predictions.predictions, axis=1)\n",
    "test_true_labels = predictions.label_ids\n",
    "\n",
    "# Classification report\n",
    "print(\"\\n📋 CLASSIFICATION REPORT (Test Set):\")\n",
    "class_names = ['negative', 'positive', 'no_impact']\n",
    "print(classification_report(test_true_labels, test_preds, target_names=class_names))\n",
    "\n",
    "# Matriz de confusión\n",
    "cm = confusion_matrix(test_true_labels, test_preds)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "plt.title('Matriz de Confusión - DeBERTa-v3-base Fine-tuned\\n(Test Set)')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# 8. GUARDADO DEL MODELO FINAL\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n8️⃣ GUARDANDO MODELO Y RESULTADOS...\")\n",
    "\n",
    "# Crear directorio de salida si no existe\n",
    "os.makedirs(config.OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Guardar el mejor modelo cargado por load_best_model_at_end\n",
    "final_model_path = f\"{config.OUTPUT_DIR}/deberta_v3_poem_sentiment_fine_tuned\"\n",
    "trainer.save_model(final_model_path)\n",
    "tokenizer.save_pretrained(final_model_path)\n",
    "\n",
    "# Guardar configuración y resultados\n",
    "results_summary = {\n",
    "    'model_name': config.MODEL_NAME,\n",
    "    'final_test_results': eval_results,\n",
    "    'class_weights': class_weights.tolist(),\n",
    "    'hyperparameters': {\n",
    "        'max_len': config.MAX_LEN,\n",
    "        'batch_size': config.BATCH_SIZE,\n",
    "        'learning_rate': config.LEARNING_RATE,\n",
    "        'epochs': config.NUM_EPOCHS,\n",
    "        'focal_gamma': config.FOCAL_GAMMA,\n",
    "        'label_smoothing': config.LABEL_SMOOTHING,\n",
    "        'r_drop_alpha': config.R_DROP_ALPHA, # Note: R-Drop not implemented in this Trainer version\n",
    "        'early_stopping_patience': config.EARLY_STOPPING_PATIENCE\n",
    "    },\n",
    "    'preprocessing': {\n",
    "        'removed_duplicates': True,\n",
    "        'normalized_quotes_hyphens': True,\n",
    "        'excluded_class_3': True,\n",
    "        'no_stemming_stopwords': True\n",
    "    }\n",
    "}\n",
    "\n",
    "import json\n",
    "with open(f\"{config.OUTPUT_DIR}/fine_tuning_results.json\", 'w') as f:\n",
    "    json.dump(results_summary, f, indent=2)\n",
    "\n",
    "print(f\"✅ Modelo guardado en: {final_model_path}\")\n",
    "print(f\"✅ Resultados guardados en: {config.OUTPUT_DIR}/fine_tuning_results.json\")\n",
    "\n",
    "# ============================================================================\n",
    "# 9. CONCLUSIONES FINALES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"🎯 CONCLUSIONES FINALES - FINE-TUNING DEBERTA-V3-BASE\")\n",
    "print(\"=\" + \"=\" * 80)\n",
    "\n",
    "print(f\"\"\"\n",
    "✅ FINE-TUNING DEBERTA-V3-BASE COMPLETADO:\n",
    "\n",
    "🔬 TÉCNICAS APLICADAS:\n",
    "   • Modelo: DeBERTa-v3-base (Fine-tuned)\n",
    "   • Preprocesamiento: Eliminación duplicados, normalización comillas/guiones, exclusión clase 'mixed'\n",
    "   • Tokenización: max_len={config.MAX_LEN}\n",
    "   • Pérdida: Focal Loss (γ={config.FOCAL_GAMMA}) con class weights\n",
    "   • Regularización: Label smoothing {config.LABEL_SMOOTHING}\n",
    "   • Entrenamiento: Hugging Face Trainer, optimizado por F1-macro\n",
    "   • Validación: Early stopping (patience={config.EARLY_STOPPING_PATIENCE})\n",
    "\n",
    "📊 RESULTADOS FINALES (Test Set):\n",
    "   • F1-Macro Test: {eval_results['eval_f1_macro']:.4f}\n",
    "   • Accuracy: {eval_results['eval_accuracy']:.4f}\n",
    "   • F1-Weighted: {eval_results['eval_f1_weighted']:.4f}\n",
    "\n",
    "🎯 OBJETIVO F1-MACRO >= 0.85:\n",
    "   {'✅ ALCANZADO' if achieved else '❌ NO ALCANZADO'} ({'+' if achieved else ''}{eval_results['eval_f1_macro'] - target_f1:+.4f})\n",
    "\n",
    "💡 RECOMENDACIONES PARA MEJORAR:\n",
    "   {'• ¡Excellent rendimiento! Considera deploy en producción' if achieved else '• Experimenta con R-Drop (requiere Custom Trainer con forward pass doble)'}\n",
    "   {'• Modelo listo para uso en producción' if achieved else '• Prueba diferentes hiperparámetros de entrenamiento (LR, batch size, weight decay)'}\n",
    "   {'• Monitorea performance en datos reales' if achieved else '• Considera data augmentation a nivel de texto o embeddings'}\n",
    "   {'• Considera fine-tuning en dominio específico' if achieved else '• Evalúa modelos más grandes (DeBERTa-v3-large) if resources permit'}\n",
    "   {'• Implementar ensemble of different checkpoints or seeds' if achieved else ''}\n",
    "\n",
    "🚀 PRÓXIMOS PASOS:\n",
    "   1. Comparar resultados with the embeddings + MLP/Ensemble approach\n",
    "   2. Perform detailed error analysis on the test set\n",
    "   3. Consider strategies for the 'mixed' class if relevant\n",
    "   4. Prepare for deploy\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"✅ PROCESO DE FINE-TUNING COMPLETADO\")\n",
    "print(\"=\" + \"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dipEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
