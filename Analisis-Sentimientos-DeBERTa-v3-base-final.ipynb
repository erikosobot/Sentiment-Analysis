{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2dd9636a",
   "metadata": {},
   "source": [
    "## 7) Discusi√≥n y conclusiones\n",
    "\n",
    "Observaciones principales:\n",
    "- El desbalance (‚âà66% `no_impact`) induce sesgos hacia la clase mayoritaria; Focal Loss + class weights elevaron F1‚Äëmacro y balancearon el aprendizaje.\n",
    "- La clase `positive` fue la m√°s desafiante (F1‚âà0.81 en Test), con confusiones hacia `no_impact` cuando el verso es m√°s descriptivo que valorativo.\n",
    "- La clase `negative` logr√≥ alta recall (‚âà1.00), se√±al de que sus marcadores son m√°s expl√≠citos.\n",
    "\n",
    "¬øFue exitoso el entrenamiento?\n",
    "- S√≠. F1‚Äëmacro Test = 0.8909 (> 0.85). Curvas por √©poca estables con early stopping; label smoothing ayud√≥ a la calibraci√≥n.\n",
    "- `max_len=128` fue suficiente para la mayor√≠a de versos.\n",
    "\n",
    "Escenarios de uso:\n",
    "- Curadur√≠a y an√°lisis de sentimiento en colecciones po√©ticas.\n",
    "- Herramientas de apoyo editorial y anal√≠tica creativa.\n",
    "- Investigaci√≥n de marcadores afectivos en poes√≠a.\n",
    "\n",
    "Limitaciones y mejoras:\n",
    "- Mejorar la clase `positive`: augmentations suaves, m√°s √©pocas con early stopping, o ensembles de seeds/√©pocas.\n",
    "- Probar modelos alternativos (DeBERTa‚Äëv3‚Äëlarge, RoBERTa‚Äëlarge) si hay GPU; distilados/quantization para despliegue eficiente.\n",
    "- Reincorporar `mixed` requerir√° m√°s datos o ajuste de p√©rdidas/umbral.\n",
    "\n",
    "Reproducibilidad y despliegue:\n",
    "- Fijar seeds y registrar versiones de dependencias.\n",
    "- Exportar checkpoint + tokenizer; opcionalmente un endpoint (FastAPI/Gradio) para demo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57c1432",
   "metadata": {},
   "source": [
    "## 6) Ejemplos de uso (inferencia)\n",
    "\n",
    "Ejemplos de entradas y etiquetas esperadas:\n",
    "- ‚ÄúThe night is dark and full of fears.‚Äù ‚Üí `negative`\n",
    "- ‚ÄúSunshine fills my heart with joy.‚Äù ‚Üí `positive`\n",
    "- ‚ÄúThe sky is blue and the grass is green.‚Äù ‚Üí `no_impact`\n",
    "\n",
    "C√≥mo aplicar (resumen):\n",
    "1) Tokenizar el verso con el tokenizer del checkpoint final.\n",
    "2) Generar tensores (m√°x. longitud 128) y pasarlos al modelo.\n",
    "3) Tomar `argmax` de los logits y mapear a `{0: negative, 1: positive, 2: no_impact}`.\n",
    "\n",
    "Nota: si se dispone del checkpoint guardado, puede a√±adirse una celda de ‚Äúdemo de inferencia‚Äù para procesar una lista de versos de ejemplo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c292a79",
   "metadata": {},
   "source": [
    "## 5) Evaluaci√≥n: F1‚Äëmacro y matriz de confusi√≥n\n",
    "\n",
    "Resultados obtenidos en Colab (Test):\n",
    "- Accuracy: 0.9135\n",
    "- F1‚ÄëMacro: 0.8909\n",
    "- F1‚ÄëWeighted: 0.9136\n",
    "\n",
    "Tabla comparativa (baselines marcados como pendientes):\n",
    "\n",
    "| Modelo                         | Features                 | F1‚Äëmacro (Val) | F1‚Äëmacro (Test) | Accuracy (Test) | Notas                                  |\n",
    "|--------------------------------|--------------------------|----------------|-----------------|-----------------|----------------------------------------|\n",
    "| TF‚ÄëIDF + LinearSVC             | n‚Äëgramas 1‚Äì2/1‚Äì3         | Pendiente      | Pendiente       | Pendiente       | Baseline cl√°sico r√°pido                 |\n",
    "| TextCNN/BiLSTM                 | Embeddings + CNN/LSTM    | Pendiente      | Pendiente       | Pendiente       | Baseline neural ligero                  |\n",
    "| DeBERTa‚Äëv3‚Äëbase (fine‚Äëtuning)  | Subword tokenizer        | 0.8903         | 0.8909          | 0.9135          | Focal Loss + weights; label smoothing   |\n",
    "\n",
    "Matriz de confusi√≥n (Test) y observaciones:\n",
    "- Negative: precision 0.86, recall 1.00 ‚Üí casi sin falsos negativos.\n",
    "- Positive: precision 0.81, recall 0.81 ‚Üí clase m√°s desafiante; confusiones con `no_impact`.\n",
    "- No_impact: precision 0.95, recall 0.91 ‚Üí muy bien capturada, consistente con su mayor√≠a.\n",
    "\n",
    "Requisito: al menos un modelo con F1‚Äëmacro ‚â• 0.85 ‚Üí Cumplido por DeBERTa‚Äëv3‚Äëbase.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902fbe62",
   "metadata": {},
   "source": [
    "## 4) Uso de arquitecturas preentrenadas\n",
    "\n",
    "- Se emple√≥ un transformador preentrenado (`microsoft/deberta‚Äëv3‚Äëbase`) con ajuste fino completo en el dataset objetivo.\n",
    "- Se cumple el requisito de que al menos una arquitectura sea entrenada/ajustada finamente.\n",
    "- Alternativas r√°pidas para comparaci√≥n: DistilBERT, BERT base, RoBERTa base (menor costo que DeBERTa, aunque t√≠picamente menor desempe√±o en este dominio).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e891c0b",
   "metadata": {},
   "source": [
    "## 3) Arquitecturas propuestas\n",
    "\n",
    "A) Cl√°sica: TF‚ÄëIDF + Linear SVM (LinearSVC)\n",
    "- Pipeline: limpieza ligera ‚Üí TF‚ÄëIDF (n‚Äëgramas 1‚Äì2/1‚Äì3) ‚Üí LinearSVC.\n",
    "- Ventajas: r√°pido, baseline fuerte en textos cortos; robusto con `class_weight='balanced'`.\n",
    "- Desventajas: no captura contexto largo ni sem√°ntica contextual.\n",
    "- Hiperpar√°metros recomendados: `max_features=50k‚Äì100k`, `ngram_range=(1,2)` o `(1,3)`, `C‚àà[0.5,2.0]`.\n",
    "\n",
    "B) Neural ‚Äúligero‚Äù: TextCNN o BiLSTM\n",
    "- Pipeline: tokenizaci√≥n simple ‚Üí embeddings (preentrenados o entrenables) ‚Üí CNN 1D (filtros 3/4/5) o BiLSTM ‚Üí densa.\n",
    "- Ventajas: CNN capta patrones locales; BiLSTM capta dependencias secuenciales.\n",
    "- Desventajas: se entrena desde cero; sensible a regularizaci√≥n y early stopping.\n",
    "- Sugerencias: `max_len=128`, `emb_dim=100‚Äì300`, `dropout=0.3‚Äì0.5`, `Adam lr‚âà1e‚Äë3`.\n",
    "\n",
    "C) Transformador preentrenado: DeBERTa‚Äëv3‚Äëbase (fine‚Äëtuning)\n",
    "- T√©cnica: Focal Loss (Œ≥=1.5) con class weights, label smoothing=0.1, early stopping.\n",
    "- Ventajas: captura sem√°ntica y contexto; resultados SOTA.\n",
    "- Desventajas: requiere GPU para tiempos razonables; mayor memoria.\n",
    "- Hiperpar√°metros usados: `batch_size=16`, `lr=2e‚Äë5`, `epochs=6`, `max_len=128`, `warmup_ratio=0.05`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea68c5a",
   "metadata": {},
   "source": [
    "## 2) Preprocesamiento y justificaci√≥n\n",
    "\n",
    "Preprocesamiento aplicado (ligero y no destructivo):\n",
    "- Normalizaci√≥n de comillas y guiones: unifica variantes Unicode a ASCII para evitar tokens raros.\n",
    "- Colapsado de espacios m√∫ltiples y recorte (trim).\n",
    "- Eliminaci√≥n de versos duplicados por coincidencia exacta dentro de cada split.\n",
    "\n",
    "Decisiones expl√≠citas:\n",
    "- Sin stemming ni stopwords: en poes√≠a, la morfolog√≠a y las llamadas ‚Äústopwords‚Äù aportan matices; removerlas suele degradar se√±ales sutiles de sentimiento.\n",
    "- Sin lowercasing forzado: los tokenizadores subword (p. ej., DeBERTa) manejan may√∫sculas; adem√°s, en poes√≠a las may√∫sculas pueden portar informaci√≥n estil√≠stica.\n",
    "\n",
    "Impacto esperado:\n",
    "- Reducir ruido tipogr√°fico sin perder sem√°ntica ni estilo del verso.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e050a2f0",
   "metadata": {},
   "source": [
    "## 1) Importar el conjunto de datos\n",
    "\n",
    "- Fuente: Hugging Face Dataset `google-research-datasets/poem_sentiment` en formato Parquet accesible v√≠a URIs `hf://`.\n",
    "- Splits usados: `train`, `validation` y `test`.\n",
    "- Exclusi√≥n: se excluye la clase `3` (mixed) en todos los splits para formular un problema de 3 clases: `0 = negative`, `1 = positive`, `2 = no_impact`.\n",
    "- Tama√±os tras la exclusi√≥n (seg√∫n la corrida en Colab):\n",
    "  - Train: 843\n",
    "  - Validation: 105\n",
    "  - Test: 104\n",
    "- Distribuci√≥n por clase (aprox.): `no_impact ~66%`, `negative ~18%`, `positive ~16%` ‚Üí dataset desbalanceado.\n",
    "\n",
    "Justificaci√≥n de la fuente y formato:\n",
    "- El acceso v√≠a `hf://` evita descargas manuales y favorece reproducibilidad.\n",
    "- Parquet es eficiente y se integra bien con `fastparquet`/`pyarrow` para lectura r√°pida.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5463409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fastparquet in c:\\users\\osorn\\anaconda3\\envs\\dipenv\\lib\\site-packages (2024.11.0)\n",
      "Requirement already satisfied: pandas>=1.5.0 in c:\\users\\osorn\\anaconda3\\envs\\dipenv\\lib\\site-packages (from fastparquet) (2.3.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\osorn\\anaconda3\\envs\\dipenv\\lib\\site-packages (from fastparquet) (2.3.4)\n",
      "Requirement already satisfied: cramjam>=2.3 in c:\\users\\osorn\\anaconda3\\envs\\dipenv\\lib\\site-packages (from fastparquet) (2.11.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\osorn\\anaconda3\\envs\\dipenv\\lib\\site-packages (from fastparquet) (2025.9.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\osorn\\anaconda3\\envs\\dipenv\\lib\\site-packages (from fastparquet) (25.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\osorn\\anaconda3\\envs\\dipenv\\lib\\site-packages (from pandas>=1.5.0->fastparquet) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\osorn\\anaconda3\\envs\\dipenv\\lib\\site-packages (from pandas>=1.5.0->fastparquet) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\osorn\\anaconda3\\envs\\dipenv\\lib\\site-packages (from pandas>=1.5.0->fastparquet) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\osorn\\anaconda3\\envs\\dipenv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.5.0->fastparquet) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install fastparquet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2246c3ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Model: microsoft/deberta-v3-base\n",
      "Max length: 128\n",
      "Batch size: 16\n",
      "Learning rate: 2e-05\n",
      "\n",
      "‚úÖ Configuraci√≥n avanzada completada\n",
      "üöÄ Listo para implementar DeBERTa-v3-base con t√©cnicas de regularizaci√≥n avanzadas\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# IMPORTS Y CONFIGURACI√ìN INICIAL - DEBERTA-V3-BASE PARA POEM_SENTIMENT\n",
    "# ============================================================================\n",
    "\n",
    "# Instala dependencias avanzadas\n",
    "#%pip install transformers torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "#%pip install datasets accelerate peft\n",
    "#%pip install scikit-learn numpy pandas matplotlib seaborn\n",
    "#%pip install nltk fastparquet\n",
    "\n",
    "# ============================================================================\n",
    "# IMPORTS DE LIBRER√çAS\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, LinearLR\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    TrainingArguments, Trainer, EarlyStoppingCallback,\n",
    "    DataCollatorWithPadding, set_seed\n",
    ")\n",
    "from datasets import Dataset as HFDataset\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Desactivar warnings molestos\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "set_seed(42)\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURACI√ìN GLOBAL\n",
    "# ============================================================================\n",
    "\n",
    "class Config:\n",
    "    # Modelo\n",
    "    MODEL_NAME = \"microsoft/deberta-v3-base\"\n",
    "    MAX_LEN = 128\n",
    "    NUM_LABELS = 3  # Excluyendo clase 3 (mixed)\n",
    "\n",
    "    # Entrenamiento\n",
    "    BATCH_SIZE = 16\n",
    "    LEARNING_RATE = 2e-5\n",
    "    NUM_EPOCHS = 6\n",
    "    WARMUP_RATIO = 0.05\n",
    "    WEIGHT_DECAY = 0.01\n",
    "\n",
    "    # Regularizaci√≥n\n",
    "    LABEL_SMOOTHING = 0.1\n",
    "    FOCAL_GAMMA = 1.5\n",
    "    R_DROP_ALPHA = 4.0\n",
    "\n",
    "    # Validaci√≥n\n",
    "    N_FOLDS = 5\n",
    "    EARLY_STOPPING_PATIENCE = 3\n",
    "\n",
    "    # Paths\n",
    "    OUTPUT_DIR = \"./deberta_results\"\n",
    "    LOGGING_DIR = \"./logs\"\n",
    "\n",
    "    # Device\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "config = Config()\n",
    "\n",
    "print(f\"Using device: {config.DEVICE}\")\n",
    "print(f\"Model: {config.MODEL_NAME}\")\n",
    "print(f\"Max length: {config.MAX_LEN}\")\n",
    "print(f\"Batch size: {config.BATCH_SIZE}\")\n",
    "print(f\"Learning rate: {config.LEARNING_RATE}\")\n",
    "\n",
    "# ============================================================================\n",
    "# PREPROCESAMIENTO AVANZADO\n",
    "# ============================================================================\n",
    "\n",
    "def advanced_text_preprocessing(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Preprocesamiento avanzado para texto po√©tico:\n",
    "    - Eliminar duplicados de versos\n",
    "    - Normalizar comillas y guiones\n",
    "    - NO stemming/stopwords (preservar matices)\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    # Normalizar comillas (usando unicode characters)\n",
    "    text = re.sub(r'[‚Äú‚Äù¬´¬ª‚Äû\"]', '\"', text) # Double quotes\n",
    "    text = re.sub(r'[‚Äò‚Äô‚Äπ‚Ä∫`¬¥]', \"'\", text) # Single quotes and backticks/accents\n",
    "\n",
    "    # Normalizar guiones\n",
    "    text = re.sub(r'[‚Äì‚Äî]', '-', text)\n",
    "\n",
    "    # Eliminar espacios m√∫ltiples\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    # Eliminar espacios al inicio/fin\n",
    "    text = text.strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "def remove_duplicate_verses(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Eliminar versos duplicados dentro del mismo dataset.\"\"\"\n",
    "    initial_len = len(df)\n",
    "    df_clean = df.drop_duplicates(subset=['verse_text'], keep='first')\n",
    "    final_len = len(df_clean)\n",
    "\n",
    "    print(f\"Removed {initial_len - final_len} duplicate verses\")\n",
    "    return df_clean\n",
    "\n",
    "# ============================================================================\n",
    "# FOCAL LOSS PARA DESBALANCE\n",
    "# ============================================================================\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"Focal Loss para manejar desbalance de clases.\"\"\"\n",
    "    def __init__(self, alpha=None, gamma=config.FOCAL_GAMMA, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none', weight=self.alpha)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = ((1 - pt) ** self.gamma) * ce_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "# ============================================================================\n",
    "# R-DROP REGULARIZATION\n",
    "# ============================================================================\n",
    "\n",
    "def r_drop_loss(logits1, logits2, targets, alpha=config.R_DROP_ALPHA):\n",
    "    \"\"\"R-Drop regularization para mejorar generalizaci√≥n.\"\"\"\n",
    "    loss1 = F.cross_entropy(logits1, targets, reduction='mean')\n",
    "    loss2 = F.cross_entropy(logits2, targets, reduction='mean')\n",
    "\n",
    "    # KL divergence entre las dos predicciones\n",
    "    kl_loss = F.kl_div(\n",
    "        F.log_softmax(logits1, dim=-1),\n",
    "        F.softmax(logits2, dim=-1),\n",
    "        reduction='batchmean'\n",
    "    )\n",
    "\n",
    "    return (loss1 + loss2) / 2 + alpha * kl_loss\n",
    "\n",
    "# ============================================================================\n",
    "# DATASET CUSTOM PARA HUGGINGFACE\n",
    "# ============================================================================\n",
    "\n",
    "class PoemSentimentDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=config.MAX_LEN):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# ============================================================================\n",
    "# CUSTOM TRAINER CON FOCAL LOSS Y R-DROP\n",
    "# ============================================================================\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, class_weights=None, use_r_drop=False, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = class_weights\n",
    "        self.use_r_drop = use_r_drop\n",
    "\n",
    "        if class_weights is not None:\n",
    "            self.criterion = FocalLoss(alpha=class_weights.to(self.model.device))\n",
    "        else:\n",
    "            self.criterion = FocalLoss()\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "\n",
    "        if self.use_r_drop:\n",
    "            # Forward pass 1\n",
    "            outputs1 = model(**inputs)\n",
    "            logits1 = outputs1.logits\n",
    "\n",
    "            # Forward pass 2 (con dropout diferente)\n",
    "            outputs2 = model(**inputs)\n",
    "            logits2 = outputs2.logits\n",
    "\n",
    "            # R-Drop loss\n",
    "            loss = r_drop_loss(logits1, logits2, labels)\n",
    "        else:\n",
    "            # Forward pass normal\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            loss = self.criterion(logits, labels)\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# ============================================================================\n",
    "# M√âTRICAS CUSTOM PARA F1 MACRO\n",
    "# ============================================================================\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    f1_macro = f1_score(labels, predictions, average='macro')\n",
    "    f1_weighted = f1_score(labels, predictions, average='weighted')\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1_macro': f1_macro,\n",
    "        'f1_weighted': f1_weighted\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# VALIDACI√ìN CON STRATIFIED K-FOLD\n",
    "# ============================================================================\n",
    "\n",
    "def stratified_kfold_training(train_texts, train_labels, config):\n",
    "    \"\"\"Entrenamiento con Stratified K-Fold para robustez.\"\"\"\n",
    "\n",
    "    # Configurar estratificaci√≥n\n",
    "    skf = StratifiedKFold(n_splits=config.N_FOLDS, shuffle=True, random_state=42)\n",
    "\n",
    "    fold_results = []\n",
    "    best_models = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(train_texts, train_labels)):\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"FOLD {fold + 1}/{config.N_FOLDS}\")\n",
    "        print(f\"{'='*50}\")\n",
    "\n",
    "        # Split data\n",
    "        fold_train_texts = [train_texts[i] for i in train_idx]\n",
    "        fold_train_labels = [train_labels[i] for i in train_idx]\n",
    "        fold_val_texts = [train_texts[i] for i in val_idx]\n",
    "        fold_val_labels = [train_labels[i] for i in val_idx]\n",
    "\n",
    "        # Calcular class weights para este fold\n",
    "        class_weights = compute_class_weight(\n",
    "            'balanced',\n",
    "            classes=np.unique(fold_train_labels),\n",
    "            y=fold_train_labels\n",
    "        )\n",
    "        class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
    "\n",
    "        # Tokenizer y modelo\n",
    "        tokenizer = AutoTokenizer.from_pretrained(config.MODEL_NAME)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            config.MODEL_NAME,\n",
    "            num_labels=config.NUM_LABELS,\n",
    "            label_smoothing=config.LABEL_SMOOTHING\n",
    "        )\n",
    "\n",
    "        # Datasets\n",
    "        train_dataset = PoemSentimentDataset(fold_train_texts, fold_train_labels, tokenizer)\n",
    "        val_dataset = PoemSentimentDataset(fold_val_texts, fold_val_labels, tokenizer)\n",
    "\n",
    "        # Training arguments\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=f\"{config.OUTPUT_DIR}/fold_{fold}\",\n",
    "            logging_dir=f\"{config.LOGGING_DIR}/fold_{fold}\",\n",
    "            num_train_epochs=config.NUM_EPOCHS,\n",
    "            per_device_train_batch_size=config.BATCH_SIZE,\n",
    "            per_device_eval_batch_size=config.BATCH_SIZE * 2,\n",
    "            learning_rate=config.LEARNING_RATE,\n",
    "            weight_decay=config.WEIGHT_DECAY,\n",
    "            warmup_ratio=config.WARMUP_RATIO,\n",
    "            lr_scheduler_type=\"cosine\",\n",
    "            save_strategy=\"epoch\",\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"f1_macro\",\n",
    "            greater_is_better=True,\n",
    "            save_total_limit=2,\n",
    "            logging_steps=50,\n",
    "            report_to=\"none\"\n",
    "        )\n",
    "\n",
    "        # Callbacks\n",
    "        early_stopping = EarlyStoppingCallback(\n",
    "            early_stopping_patience=config.EARLY_STOPPING_PATIENCE\n",
    "        )\n",
    "\n",
    "        # Trainer\n",
    "        trainer = CustomTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            tokenizer=tokenizer,\n",
    "            compute_metrics=compute_metrics,\n",
    "            callbacks=[early_stopping],\n",
    "            class_weights=class_weights,\n",
    "            use_r_drop=True\n",
    "        )\n",
    "\n",
    "        # Entrenar\n",
    "        trainer.train()\n",
    "\n",
    "        # Evaluar en validation\n",
    "        eval_results = trainer.evaluate()\n",
    "        fold_results.append(eval_results)\n",
    "\n",
    "        # Guardar mejor modelo de este fold\n",
    "        best_models.append(trainer.model)\n",
    "\n",
    "        print(f\"Fold {fold + 1} - F1 Macro: {eval_results['eval_f1_macro']:.4f}\")\n",
    "\n",
    "    return fold_results, best_models\n",
    "\n",
    "# ============================================================================\n",
    "# ENSEMBLE DE CHECKPOINTS\n",
    "# ============================================================================\n",
    "\n",
    "def ensemble_predictions(models, tokenizer, test_texts, config):\n",
    "    \"\"\"Ensamblar predicciones de m√∫ltiples modelos.\"\"\"\n",
    "    all_predictions = []\n",
    "\n",
    "    test_dataset = PoemSentimentDataset(test_texts, [0]*len(test_texts), tokenizer)\n",
    "\n",
    "    for model in models:\n",
    "        model.eval()\n",
    "        predictions = []\n",
    "\n",
    "        dataloader = DataLoader(\n",
    "            test_dataset,\n",
    "            batch_size=config.BATCH_SIZE * 4,\n",
    "            shuffle=False\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                input_ids = batch['input_ids'].to(config.DEVICE)\n",
    "                attention_mask = batch['attention_mask'].to(config.DEVICE)\n",
    "\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                logits = outputs.logits\n",
    "                predictions.extend(logits.cpu().numpy())\n",
    "\n",
    "        all_predictions.append(np.array(predictions))\n",
    "\n",
    "    # Promediar logits\n",
    "    ensemble_logits = np.mean(all_predictions, axis=0)\n",
    "    ensemble_predictions = np.argmax(ensemble_logits, axis=1)\n",
    "\n",
    "    return ensemble_predictions, ensemble_logits\n",
    "\n",
    "print(\"\\n‚úÖ Configuraci√≥n avanzada completada\")\n",
    "print(\"üöÄ Listo para implementar DeBERTa-v3-base con t√©cnicas de regularizaci√≥n avanzadas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612b7321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# IMPLEMENTACI√ìN COMPLETA: DEBERTA-V3-BASE PARA POEM_SENTIMENT\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üöÄ IMPLEMENTACI√ìN DEBERTA-V3-BASE PARA POEM_SENTIMENT\")\n",
    "print(\"=\" + \"=\" * 80)\n",
    "\n",
    "# ============================================================================\n",
    "# 1. CARGA Y PREPROCESAMIENTO DE DATOS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n1Ô∏è‚É£ CARGANDO Y PREPROCESANDO DATOS...\")\n",
    "\n",
    "# Cargar datos\n",
    "splits = {\n",
    "    \"train\": \"data/train-00000-of-00001.parquet\",\n",
    "    \"validation\": \"data/validation-00000-of-00001.parquet\",\n",
    "    \"test\": \"data/test-00000-of-00001.parquet\",\n",
    "}\n",
    "base_uri = \"hf://datasets/google-research-datasets/poem_sentiment/\"\n",
    "parquet_engine = \"fastparquet\"\n",
    "\n",
    "df_train = pd.read_parquet(base_uri + splits[\"train\"], engine=parquet_engine)\n",
    "df_validation = pd.read_parquet(base_uri + splits[\"validation\"], engine=parquet_engine)\n",
    "df_test = pd.read_parquet(base_uri + splits[\"test\"], engine=parquet_engine)\n",
    "\n",
    "print(f\"Datos originales:\")\n",
    "print(f\"  Train: {len(df_train)} muestras\")\n",
    "print(f\"  Validation: {len(df_validation)} muestras\")\n",
    "print(f\"  Test: {len(df_test)} muestras\")\n",
    "\n",
    "# Preprocesamiento avanzado\n",
    "print(\"\\nüîß Aplicando preprocesamiento avanzado...\")\n",
    "\n",
    "def advanced_text_preprocessing(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Preprocesamiento avanzado para texto po√©tico:\n",
    "    - Normalizar comillas y guiones\n",
    "    - Eliminar espacios m√∫ltiples\n",
    "    - Eliminar espacios al inicio/fin\n",
    "    - NO stemming/stopwords (preservar matices)\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    # Normalizar comillas (usando unicode characters)\n",
    "    text = re.sub(r'[‚Äú‚Äù¬´¬ª‚Äû\"]', '\"', text) # Double quotes\n",
    "    text = re.sub(r'[‚Äò‚Äô‚Äπ‚Ä∫`¬¥]', \"'\", text) # Single quotes and backticks/accents\n",
    "\n",
    "    # Normalizar guiones\n",
    "    text = re.sub(r'[‚Äì‚Äî]', '-', text)\n",
    "\n",
    "    # Eliminar espacios m√∫ltiples\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    # Eliminar espacios al inicio/fin\n",
    "    text = text.strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "def remove_duplicate_verses(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Eliminar versos duplicados dentro del mismo dataset.\"\"\"\n",
    "    initial_len = len(df)\n",
    "    df_clean = df.drop_duplicates(subset=['verse_text'], keep='first')\n",
    "    final_len = len(df_clean)\n",
    "    if initial_len - final_len > 0:\n",
    "        print(f\"Removed {initial_len - final_len} duplicate verses\")\n",
    "    return df_clean\n",
    "\n",
    "\n",
    "# Eliminar duplicados\n",
    "df_train = remove_duplicate_verses(df_train)\n",
    "df_validation = remove_duplicate_verses(df_validation)\n",
    "df_test = remove_duplicate_verses(df_test)\n",
    "\n",
    "# Aplicar preprocesamiento de texto\n",
    "df_train['verse_text'] = df_train['verse_text'].apply(advanced_text_preprocessing)\n",
    "df_validation['verse_text'] = df_validation['verse_text'].apply(advanced_text_preprocessing)\n",
    "df_test['verse_text'] = df_test['verse_text'].apply(advanced_text_preprocessing)\n",
    "\n",
    "# EXCLUIR CLASE 3 (mixed) COMPLETAMENTE\n",
    "print(\"\\n‚ùå Excluyendo clase 'mixed' (3) de todos los splits...\")\n",
    "mask_train = df_train['label'] != 3\n",
    "mask_val = df_validation['label'] != 3\n",
    "mask_test = df_test['label'] != 3\n",
    "\n",
    "df_train = df_train[mask_train].reset_index(drop=True)\n",
    "df_validation = df_validation[mask_val].reset_index(drop=True)\n",
    "df_test = df_test[mask_test].reset_index(drop=True)\n",
    "\n",
    "print(f\"Datos despu√©s de excluir clase 3:\")\n",
    "print(f\"  Train: {len(df_train)} muestras\")\n",
    "print(f\"  Validation: {len(df_validation)} muestras\")\n",
    "print(f\"  Test: {len(df_test)} muestras\")\n",
    "\n",
    "# Preparar arrays finales\n",
    "train_texts = df_train['verse_text'].tolist()\n",
    "train_labels = df_train['label'].values\n",
    "\n",
    "val_texts = df_validation['verse_text'].tolist()\n",
    "val_labels = df_validation['label'].values\n",
    "\n",
    "test_texts = df_test['verse_text'].tolist()\n",
    "test_labels = df_test['label'].values\n",
    "\n",
    "# Verificar distribuci√≥n final\n",
    "print(\"\\nüìä Distribuci√≥n final de clases:\")\n",
    "for name, labels in [(\"Train\", train_labels), (\"Validation\", val_labels), (\"Test\", test_labels)]:\n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    print(f\"  {name}: \", end=\"\")\n",
    "    for label, count in zip(unique, counts):\n",
    "        class_name = {0: 'negative', 1: 'positive', 2: 'no_impact'}.get(label, f'class_{label}')\n",
    "        pct = 100 * count / len(labels)\n",
    "        print(f\"{class_name}={count}({pct:.1f}%) \", end=\"\")\n",
    "    print()\n",
    "\n",
    "# ============================================================================\n",
    "# 2. TOKENIZACI√ìN Y DATASET PREPARACI√ìN (HUGGING FACE)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n2Ô∏è‚É£ TOKENIZANDO Y PREPARANDO DATASETS (Hugging Face)...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.MODEL_NAME)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=config.MAX_LEN)\n",
    "\n",
    "# Convertir a Hugging Face Datasets\n",
    "train_hf = HFDataset.from_pandas(pd.DataFrame({'text': train_texts, 'label': train_labels}))\n",
    "val_hf = HFDataset.from_pandas(pd.DataFrame({'text': val_texts, 'label': val_labels}))\n",
    "test_hf = HFDataset.from_pandas(pd.DataFrame({'text': test_texts, 'label': test_labels}))\n",
    "\n",
    "# Aplicar tokenizaci√≥n\n",
    "tokenized_train = train_hf.map(tokenize_function, batched=True)\n",
    "tokenized_val = val_hf.map(tokenize_function, batched=True)\n",
    "tokenized_test = test_hf.map(tokenize_function, batched=True)\n",
    "\n",
    "print(\"‚úÖ Datasets tokenizados.\")\n",
    "print(tokenized_train)\n",
    "print(tokenized_val)\n",
    "print(tokenized_test)\n",
    "\n",
    "# ============================================================================\n",
    "# 3. AN√ÅLISIS DE DESBALANCE Y CLASS WEIGHTS (para Trainer)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n3Ô∏è‚É£ ANALIZANDO DESBALANCE Y CALCULANDO CLASS WEIGHTS...\")\n",
    "\n",
    "# Calcular class weights globales\n",
    "class_weights = compute_class_weight(\n",
    "    'balanced',\n",
    "    classes=np.unique(train_labels),\n",
    "    y=train_labels\n",
    ")\n",
    "\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(config.DEVICE)\n",
    "print(f\"Class weights (for Trainer): {class_weights_tensor.cpu().numpy()}\")\n",
    "print(f\"  ‚Ä¢ Negative (0): {class_weights_tensor[0].item():.3f}\")\n",
    "print(f\"  ‚Ä¢ Positive (1): {class_weights_tensor[1].item():.3f}\")\n",
    "print(f\"  ‚Ä¢ No_impact (2): {class_weights_tensor[2].item():.3f}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 4. CONFIGURACI√ìN DEL MODELO Y TRAINER\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n4Ô∏è‚É£ CONFIGURANDO MODELO Y TRAINER...\")\n",
    "\n",
    "# Cargar modelo pre-entrenado\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    config.MODEL_NAME,\n",
    "    num_labels=config.NUM_LABELS\n",
    ")\n",
    "\n",
    "# Custom Trainer con Focal Loss\n",
    "class CustomTrainerWithFocalLoss(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Aplicar Focal Loss con class weights\n",
    "        criterion = FocalLoss(alpha=class_weights_tensor)\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# Define SEED\n",
    "SEED = 42\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=config.OUTPUT_DIR,\n",
    "    logging_dir=config.LOGGING_DIR,\n",
    "    num_train_epochs=config.NUM_EPOCHS,\n",
    "    per_device_train_batch_size=config.BATCH_SIZE,\n",
    "    per_device_eval_batch_size=config.BATCH_SIZE * 2,\n",
    "    learning_rate=config.LEARNING_RATE,\n",
    "    weight_decay=config.WEIGHT_DECAY,\n",
    "    warmup_ratio=config.WARMUP_RATIO,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\", # Corrected parameter name\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_macro\", # Optimizar por F1-macro\n",
    "    greater_is_better=True,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=50,\n",
    "    report_to=\"none\",\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStoppingCallback(\n",
    "    early_stopping_patience=config.EARLY_STOPPING_PATIENCE\n",
    ")\n",
    "\n",
    "# Inicializar Trainer\n",
    "trainer = CustomTrainerWithFocalLoss(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[early_stopping],\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer) # Usar DataCollator\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Modelo y Trainer configurados.\")\n",
    "\n",
    "# ============================================================================\n",
    "# 5. ENTRENAMIENTO DEL MODELO\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n5Ô∏è‚É£ ENTRENANDO MODELO...\")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(\"‚úÖ Entrenamiento completado.\")\n",
    "\n",
    "# ============================================================================\n",
    "# 6. EVALUACI√ìN FINAL EN TEST SET\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n6Ô∏è‚É£ EVALUACI√ìN FINAL EN TEST SET...\")\n",
    "\n",
    "eval_val_results = trainer.evaluate(eval_dataset=tokenized_val)\n",
    "\n",
    "print(\"\\nüéØ RESULTADOS FINALES EN eval_val_results SET:\")\n",
    "print(f\"  Accuracy:    {eval_val_results['eval_accuracy']:.4f}\")\n",
    "print(f\"  F1-Macro:    {eval_val_results['eval_f1_macro']:.4f}\")\n",
    "print(f\"  F1-Weighted: {eval_val_results['eval_f1_weighted']:.4f}\")\n",
    "\n",
    "#\n",
    "\n",
    "print(\"\\n6Ô∏è‚É£ EVALUACI√ìN FINAL EN TEST SET...\")\n",
    "\n",
    "eval_results = trainer.evaluate(eval_dataset=tokenized_test)\n",
    "\n",
    "print(\"\\nüéØ RESULTADOS FINALES EN TEST SET:\")\n",
    "print(f\"  Accuracy:    {eval_results['eval_accuracy']:.4f}\")\n",
    "print(f\"  F1-Macro:    {eval_results['eval_f1_macro']:.4f}\")\n",
    "print(f\"  F1-Weighted: {eval_results['eval_f1_weighted']:.4f}\")\n",
    "\n",
    "# Verificar objetivo\n",
    "target_f1 = 0.85\n",
    "achieved = eval_results['eval_f1_macro'] >= target_f1\n",
    "\n",
    "if achieved:\n",
    "    print(f\"\\nüéâüéâüéâ ¬°OBJETIVO ALCANZADO! F1-Macro >= {target_f1}\")\n",
    "else:\n",
    "    gap = target_f1 - eval_results['eval_f1_macro']\n",
    "    print(f\"\\n‚è≥ OBJETIVO NO ALCANZADO. Gap: {gap:.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 7. AN√ÅLISIS DETALLADO Y VISUALIZACIONES (Test Set)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n7Ô∏è‚É£ AN√ÅLISIS DETALLADO Y VISUALIZACIONES (Test Set)...\")\n",
    "\n",
    "# Obtener predicciones en test\n",
    "predictions = trainer.predict(tokenized_test)\n",
    "test_preds = np.argmax(predictions.predictions, axis=1)\n",
    "test_true_labels = predictions.label_ids\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nüìã CLASSIFICATION REPORT (Test Set):\")\n",
    "class_names = ['negative', 'positive', 'no_impact']\n",
    "print(classification_report(test_true_labels, test_preds, target_names=class_names))\n",
    "\n",
    "# Matriz de confusi√≥n\n",
    "cm = confusion_matrix(test_true_labels, test_preds)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "plt.title('Matriz de Confusi√≥n - DeBERTa-v3-base Fine-tuned\\n(Test Set)')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# 8. GUARDADO DEL MODELO FINAL\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n8Ô∏è‚É£ GUARDANDO MODELO Y RESULTADOS...\")\n",
    "\n",
    "# Crear directorio de salida si no existe\n",
    "os.makedirs(config.OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Guardar el mejor modelo cargado por load_best_model_at_end\n",
    "final_model_path = f\"{config.OUTPUT_DIR}/deberta_v3_poem_sentiment_fine_tuned\"\n",
    "trainer.save_model(final_model_path)\n",
    "tokenizer.save_pretrained(final_model_path)\n",
    "\n",
    "# Guardar configuraci√≥n y resultados\n",
    "results_summary = {\n",
    "    'model_name': config.MODEL_NAME,\n",
    "    'final_test_results': eval_results,\n",
    "    'class_weights': class_weights.tolist(),\n",
    "    'hyperparameters': {\n",
    "        'max_len': config.MAX_LEN,\n",
    "        'batch_size': config.BATCH_SIZE,\n",
    "        'learning_rate': config.LEARNING_RATE,\n",
    "        'epochs': config.NUM_EPOCHS,\n",
    "        'focal_gamma': config.FOCAL_GAMMA,\n",
    "        'label_smoothing': config.LABEL_SMOOTHING,\n",
    "        'r_drop_alpha': config.R_DROP_ALPHA, # Note: R-Drop not implemented in this Trainer version\n",
    "        'early_stopping_patience': config.EARLY_STOPPING_PATIENCE\n",
    "    },\n",
    "    'preprocessing': {\n",
    "        'removed_duplicates': True,\n",
    "        'normalized_quotes_hyphens': True,\n",
    "        'excluded_class_3': True,\n",
    "        'no_stemming_stopwords': True\n",
    "    }\n",
    "}\n",
    "\n",
    "import json\n",
    "with open(f\"{config.OUTPUT_DIR}/fine_tuning_results.json\", 'w') as f:\n",
    "    json.dump(results_summary, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Modelo guardado en: {final_model_path}\")\n",
    "print(f\"‚úÖ Resultados guardados en: {config.OUTPUT_DIR}/fine_tuning_results.json\")\n",
    "\n",
    "# ============================================================================\n",
    "# 9. CONCLUSIONES FINALES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üéØ CONCLUSIONES FINALES - FINE-TUNING DEBERTA-V3-BASE\")\n",
    "print(\"=\" + \"=\" * 80)\n",
    "\n",
    "print(f\"\"\"\n",
    "‚úÖ FINE-TUNING DEBERTA-V3-BASE COMPLETADO:\n",
    "\n",
    "üî¨ T√âCNICAS APLICADAS:\n",
    "   ‚Ä¢ Modelo: DeBERTa-v3-base (Fine-tuned)\n",
    "   ‚Ä¢ Preprocesamiento: Eliminaci√≥n duplicados, normalizaci√≥n comillas/guiones, exclusi√≥n clase 'mixed'\n",
    "   ‚Ä¢ Tokenizaci√≥n: max_len={config.MAX_LEN}\n",
    "   ‚Ä¢ P√©rdida: Focal Loss (Œ≥={config.FOCAL_GAMMA}) con class weights\n",
    "   ‚Ä¢ Regularizaci√≥n: Label smoothing {config.LABEL_SMOOTHING}\n",
    "   ‚Ä¢ Entrenamiento: Hugging Face Trainer, optimizado por F1-macro\n",
    "   ‚Ä¢ Validaci√≥n: Early stopping (patience={config.EARLY_STOPPING_PATIENCE})\n",
    "\n",
    "üìä RESULTADOS FINALES (Test Set):\n",
    "   ‚Ä¢ F1-Macro Test: {eval_results['eval_f1_macro']:.4f}\n",
    "   ‚Ä¢ Accuracy: {eval_results['eval_accuracy']:.4f}\n",
    "   ‚Ä¢ F1-Weighted: {eval_results['eval_f1_weighted']:.4f}\n",
    "\n",
    "üéØ OBJETIVO F1-MACRO >= 0.85:\n",
    "   {'‚úÖ ALCANZADO' if achieved else '‚ùå NO ALCANZADO'} ({'+' if achieved else ''}{eval_results['eval_f1_macro'] - target_f1:+.4f})\n",
    "\n",
    "üí° RECOMENDACIONES PARA MEJORAR:\n",
    "   {'‚Ä¢ ¬°Excellent rendimiento! Considera deploy en producci√≥n' if achieved else '‚Ä¢ Experimenta con R-Drop (requiere Custom Trainer con forward pass doble)'}\n",
    "   {'‚Ä¢ Modelo listo para uso en producci√≥n' if achieved else '‚Ä¢ Prueba diferentes hiperpar√°metros de entrenamiento (LR, batch size, weight decay)'}\n",
    "   {'‚Ä¢ Monitorea performance en datos reales' if achieved else '‚Ä¢ Considera data augmentation a nivel de texto o embeddings'}\n",
    "   {'‚Ä¢ Considera fine-tuning en dominio espec√≠fico' if achieved else '‚Ä¢ Eval√∫a modelos m√°s grandes (DeBERTa-v3-large) if resources permit'}\n",
    "   {'‚Ä¢ Implementar ensemble of different checkpoints or seeds' if achieved else ''}\n",
    "\n",
    "üöÄ PR√ìXIMOS PASOS:\n",
    "   1. Comparar resultados with the embeddings + MLP/Ensemble approach\n",
    "   2. Perform detailed error analysis on the test set\n",
    "   3. Consider strategies for the 'mixed' class if relevant\n",
    "   4. Prepare for deploy\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ PROCESO DE FINE-TUNING COMPLETADO\")\n",
    "print(\"=\" + \"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dipEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
